{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person Re-ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-10T10:17:34.803499Z",
     "iopub.status.busy": "2026-02-10T10:17:34.802815Z",
     "iopub.status.idle": "2026-02-10T10:17:41.560184Z",
     "shell.execute_reply": "2026-02-10T10:17:41.559396Z",
     "shell.execute_reply.started": "2026-02-10T10:17:34.803457Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.io\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision import transforms\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:17:51.805827Z",
     "iopub.status.busy": "2026-02-10T10:17:51.804987Z",
     "iopub.status.idle": "2026-02-10T10:17:56.417254Z",
     "shell.execute_reply": "2026-02-10T10:17:56.416517Z",
     "shell.execute_reply.started": "2026-02-10T10:17:51.805781Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtommaso-perniola\u001b[0m (\u001b[33munibo-ai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import wandb\n",
    "\n",
    "!pip install -q wandb\n",
    "!wandb login\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"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\"\n",
    "#wandb.login(key=os.environ[\"WANDB_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:18:04.148242Z",
     "iopub.status.busy": "2026-02-10T10:18:04.147648Z",
     "iopub.status.idle": "2026-02-10T10:18:04.400185Z",
     "shell.execute_reply": "2026-02-10T10:18:04.399437Z",
     "shell.execute_reply.started": "2026-02-10T10:18:04.148209Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good, a GPU is available.\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "  print(\"All good, a GPU is available.\")\n",
    "  device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "  print(\"Please set GPU via Edit -> Notebook Settings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility & deterministic mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:18:08.289415Z",
     "iopub.status.busy": "2026-02-10T10:18:08.289121Z",
     "iopub.status.idle": "2026-02-10T10:18:08.300468Z",
     "shell.execute_reply": "2026-02-10T10:18:08.299773Z",
     "shell.execute_reply.started": "2026-02-10T10:18:08.289386Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fix_random(seed: int) -> None:\n",
    "    \"\"\"Fix all the possible sources of randomness.\n",
    "\n",
    "    Args:\n",
    "        seed: the seed to use.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = 42\n",
    "fix_random(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and train/val/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:18:32.952752Z",
     "iopub.status.busy": "2026-02-10T10:18:32.952422Z",
     "iopub.status.idle": "2026-02-10T10:18:32.981581Z",
     "shell.execute_reply": "2026-02-10T10:18:32.980701Z",
     "shell.execute_reply.started": "2026-02-10T10:18:32.952723Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Dict, Any, List, Set\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import functional as FT\n",
    "\n",
    "class PRW_dataset(Dataset):\n",
    "    @staticmethod\n",
    "    def _load_mat_list(mat_path: Path, key: str):\n",
    "        d = scipy.io.loadmat(mat_path)\n",
    "        if key not in d:\n",
    "            raise KeyError(f\"Key '{key}' not found in {mat_path}. Keys: {list(d.keys())}\")\n",
    "        arr = d[key]\n",
    "        return [x[0].item() for x in arr]  # e.g. 'c1s1_000151'\n",
    "\n",
    "    @classmethod\n",
    "    def find_train_test_data(cls, frames_path: Path, train_frames_path: Path, test_frames_path: Path):\n",
    "        frames = [p.stem for p in sorted(frames_path.glob(\"*.jpg\"))]  # 'c1s1_000151'\n",
    "        frames_set = set(frames)\n",
    "\n",
    "        train_names = cls._load_mat_list(train_frames_path, \"img_index_train\")\n",
    "        test_names  = cls._load_mat_list(test_frames_path,  \"img_index_test\")\n",
    "\n",
    "        train_set = set(train_names)\n",
    "        test_set  = set(test_names)\n",
    "\n",
    "        list_train_frames = [f for f in frames if f in train_set]\n",
    "        list_test_frames  = [f for f in frames if f in test_set]\n",
    "\n",
    "        if len(list_train_frames) != len(train_names):\n",
    "            missing = [n for n in train_names if n not in frames_set]\n",
    "            raise AssertionError(f\"Train mismatch: matched={len(list_train_frames)} vs mat={len(train_names)}. Missing (first 10): {missing[:10]}\")\n",
    "        if len(list_test_frames) != len(test_names):\n",
    "            missing = [n for n in test_names if n not in frames_set]\n",
    "            raise AssertionError(f\"Test mismatch: matched={len(list_test_frames)} vs mat={len(test_names)}. Missing (first 10): {missing[:10]}\")\n",
    "\n",
    "        return list_train_frames, list_test_frames\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        frames_path: Path,\n",
    "        path_annotations: Path,\n",
    "        train_frames_path: Path,\n",
    "        test_frames_path: Path,\n",
    "        split: str = \"train\",\n",
    "        img_transform=None,\n",
    "        filter_invalid_ids: bool = False,   # if True: keep only ids > 0\n",
    "        allowed_pids: Optional[Set[int]] = None,  # keep only these IDs (positive ids)\n",
    "        drop_empty: bool = False,           # if True: drop images that have zero boxes after filtering\n",
    "    ):\n",
    "        self.img_transform = img_transform\n",
    "        self.filter_invalid_ids = filter_invalid_ids\n",
    "        self.allowed_pids = allowed_pids\n",
    "        self.drop_empty = drop_empty\n",
    "\n",
    "        split = split.lower()\n",
    "        if split not in {\"train\", \"test\"}:\n",
    "            raise ValueError(\"split must be 'train' or 'test'\")\n",
    "\n",
    "        train_frames, test_frames = self.find_train_test_data(frames_path, train_frames_path, test_frames_path)\n",
    "        allowed_frames = set(train_frames if split == \"train\" else test_frames)\n",
    "\n",
    "        self.images = [p for p in sorted(frames_path.glob(\"*.jpg\")) if p.stem in allowed_frames]\n",
    "\n",
    "        # annotations are named like: c1s1_002876.jpg.mat\n",
    "        annots = {p.stem: p for p in path_annotations.rglob(\"*.mat\")}  # key: 'c1s1_002876.jpg'\n",
    "\n",
    "        # building tuples (img, bbox)\n",
    "        pairs = []\n",
    "        for img_path in self.images:\n",
    "            ann_key = img_path.name  # e.g. 'c1s1_002876.jpg'\n",
    "            ann_path = annots.get(ann_key)\n",
    "            if ann_path is None:\n",
    "                raise RuntimeError(f\"Missing annotation for frame {img_path.name}\")\n",
    "            pairs.append((img_path, ann_path))\n",
    "\n",
    "        # pre-loading .mat files for bboxes\n",
    "        self.box_cache = {}\n",
    "        for img_path, ann_path in pairs:\n",
    "            mat = scipy.io.loadmat(ann_path)\n",
    "            arr = mat.get(\"box_new\", mat.get(\"anno_file\", mat.get(\"box\", None)))\n",
    "\n",
    "            if arr is None:\n",
    "                boxes = np.zeros((0, 4), np.float32)\n",
    "                ids   = np.zeros((0,), np.int64)\n",
    "            else:\n",
    "                arr = np.asarray(arr).reshape(-1, 5)\n",
    "                ids = arr[:, 0].astype(np.int64)\n",
    "                x = arr[:, 1].astype(np.float32)\n",
    "                y = arr[:, 2].astype(np.float32)\n",
    "                w = arr[:, 3].astype(np.float32)\n",
    "                h = arr[:, 4].astype(np.float32)\n",
    "                boxes = np.stack([x, y, x + w, y + h], axis=1).astype(np.float32)\n",
    "\n",
    "            self.box_cache[str(ann_path)] = (boxes, ids)\n",
    "\n",
    "        # Now build self.pairs, optionally filtering boxes by allowed IDs and dropping empty images\n",
    "        self.pairs = []\n",
    "        for img_path, ann_path in pairs:\n",
    "            boxes_np, ids_np = self.box_cache[str(ann_path)]\n",
    "\n",
    "            # filter invalid ids (e.g. -2 distractors, 0 background)\n",
    "            if self.filter_invalid_ids:\n",
    "                keep = ids_np > 0\n",
    "                boxes_np = boxes_np[keep]\n",
    "                ids_np = ids_np[keep]\n",
    "\n",
    "            # filter by allowed_pids (ID-disjoint train/val)\n",
    "            if self.allowed_pids is not None:\n",
    "                keep = np.isin(ids_np, np.array(list(self.allowed_pids), dtype=np.int64))\n",
    "                boxes_np = boxes_np[keep]\n",
    "                ids_np = ids_np[keep]\n",
    "\n",
    "            # update cache (filtered view)\n",
    "            self.box_cache[str(ann_path)] = (boxes_np, ids_np)\n",
    "\n",
    "            if self.drop_empty and boxes_np.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            self.pairs.append((img_path, ann_path))\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        img_path, ann_path = self.pairs[idx]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        boxes_np, ids_np = self.box_cache[str(ann_path)]\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.as_tensor(boxes_np, dtype=torch.float32),\n",
    "            \"labels\": torch.ones((boxes_np.shape[0],), dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([idx], dtype=torch.int64),\n",
    "            \"person_id\": torch.as_tensor(ids_np, dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        if self.img_transform is not None:\n",
    "            img = self.img_transform(img)\n",
    "        if not torch.is_tensor(img):\n",
    "            img = FT.to_tensor(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def unique_positive_pids(self) -> Set[int]:\n",
    "        \"\"\"Collect unique IDs > 0 present in this dataset view (after any filtering).\"\"\"\n",
    "        s = set()\n",
    "        for _, ann_path in self.pairs:\n",
    "            _, ids_np = self.box_cache[str(ann_path)]\n",
    "            for pid in ids_np.tolist():\n",
    "                if pid > 0:\n",
    "                    s.add(int(pid))\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:18:51.495809Z",
     "iopub.status.busy": "2026-02-10T10:18:51.495501Z",
     "iopub.status.idle": "2026-02-10T10:18:51.500020Z",
     "shell.execute_reply": "2026-02-10T10:18:51.499212Z",
     "shell.execute_reply.started": "2026-02-10T10:18:51.495776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "path_imgs = Path(\"/kaggle/input/prw-person-re-identification-in-the-wild/frames\")\n",
    "path_annot = Path(\"/kaggle/input/prw-person-re-identification-in-the-wild/annotations\")\n",
    "train_frames_mat = Path(\"/kaggle/input/prw-person-re-identification-in-the-wild/frame_train.mat\")\n",
    "test_frames_mat = Path(\"/kaggle/input/prw-person-re-identification-in-the-wild/frame_test.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T11:56:19.804747Z",
     "iopub.status.busy": "2026-02-10T11:56:19.804098Z",
     "iopub.status.idle": "2026-02-10T11:56:58.641283Z",
     "shell.execute_reply": "2026-02-10T11:56:58.640666Z",
     "shell.execute_reply.started": "2026-02-10T11:56:19.804713Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1) FULL train for detection (100% frame_train.mat, no ID filtering)\n",
    "det_train_ds = PRW_dataset(\n",
    "    frames_path=path_imgs,\n",
    "    path_annotations=path_annot,\n",
    "    train_frames_path=train_frames_mat,\n",
    "    test_frames_path=test_frames_mat,\n",
    "    split=\"train\",\n",
    "    img_transform=None,\n",
    "    filter_invalid_ids=False,   # detection: keep -2\n",
    "    allowed_pids=None,          \n",
    "    drop_empty=False,\n",
    ")\n",
    "\n",
    "# 2) ReID train/val + test (ID-disjoint)\n",
    "test_ds = PRW_dataset(\n",
    "    frames_path=path_imgs,\n",
    "    path_annotations=path_annot,\n",
    "    train_frames_path=train_frames_mat,\n",
    "    test_frames_path=test_frames_mat,\n",
    "    split=\"test\",\n",
    "    img_transform=None,\n",
    "    filter_invalid_ids=True,   # ReID: keep only IDs > 0\n",
    "    allowed_pids=None,        # no filtering here, we'll split by ID below\n",
    "    drop_empty=True,         # drop images that have zero boxes after filtering\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T11:56:58.642676Z",
     "iopub.status.busy": "2026-02-10T11:56:58.642409Z",
     "iopub.status.idle": "2026-02-10T11:56:58.647659Z",
     "shell.execute_reply": "2026-02-10T11:56:58.646917Z",
     "shell.execute_reply.started": "2026-02-10T11:56:58.642655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # https://discuss.pytorch.org/t/dataloader-gives-stack-expects-each-tensor-to-be-equal-size-due-to-different-image-has-different-objects-number/91941/4\n",
    "    return tuple(zip(*batch)) #unpacks the batch and groups the individual elements together based on their position.\n",
    "    #this is useful for datasets where each item contains multiple outputs (e.g., image and targets) of varying sizes.\n",
    "\n",
    "num_workers = 4\n",
    "batch_size = 2\n",
    "\n",
    "data_loader_train = DataLoader(\n",
    "    det_train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "data_loader_test = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed strategy: Two-step person search\n",
    "We will treat detection and re-id as two distinct tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:19:37.791049Z",
     "iopub.status.busy": "2026-02-10T10:19:37.790726Z",
     "iopub.status.idle": "2026-02-10T10:19:37.821132Z",
     "shell.execute_reply": "2026-02-10T10:19:37.820582Z",
     "shell.execute_reply.started": "2026-02-10T10:19:37.791022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load predictions\n",
    "with open(\"/kaggle/input/detection-weights/test_preds.pkl\", \"rb\") as fp:\n",
    "    test_detections = pickle.load(fp)\n",
    "\n",
    "# load predictions\n",
    "with open(\"/kaggle/input/detection-weights/train_preds.pkl\", \"rb\") as fp:\n",
    "    train_detections = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:19:42.428865Z",
     "iopub.status.busy": "2026-02-10T10:19:42.428554Z",
     "iopub.status.idle": "2026-02-10T10:19:42.437743Z",
     "shell.execute_reply": "2026-02-10T10:19:42.436992Z",
     "shell.execute_reply.started": "2026-02-10T10:19:42.428838Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test detections: 6112\n",
      "Per-img\n",
      "- min num_det: 1\n",
      "- mean num_det: 5\n",
      "- max num_det: 23\n",
      "\n",
      "Total val detections: 5704\n",
      "Per-img\n",
      "- min num_det: 1\n",
      "- mean num_det: 3\n",
      "- max num_det: 14\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "num_detections = [test_detections[i].shape[0] for i in range(len(test_detections))]\n",
    "print(f\"Total test detections: {len(test_detections)}\")  # should be len(test_ds)\n",
    "print(\"Per-img\")\n",
    "print(\"- min num_det:\", min(num_detections))\n",
    "print(\"- mean num_det:\", int(sum(num_detections) / len(num_detections)))\n",
    "print(\"- max num_det:\", max(num_detections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-IDentification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:19:52.766414Z",
     "iopub.status.busy": "2026-02-10T10:19:52.766119Z",
     "iopub.status.idle": "2026-02-10T10:19:52.770844Z",
     "shell.execute_reply": "2026-02-10T10:19:52.770209Z",
     "shell.execute_reply.started": "2026-02-10T10:19:52.766390Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Callable, Dict, List, Tuple\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.amp import autocast\n",
    "from typing import Callable\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as FT\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random, numpy as np\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:20:01.484511Z",
     "iopub.status.busy": "2026-02-10T10:20:01.483897Z",
     "iopub.status.idle": "2026-02-10T10:20:01.502556Z",
     "shell.execute_reply": "2026-02-10T10:20:01.501862Z",
     "shell.execute_reply.started": "2026-02-10T10:20:01.484477Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def parse_cam_id_from_stem(stem: str) -> int:\n",
    "    \"\"\"\n",
    "    PRW frame names look like: c1s3_016471 (stem)\n",
    "    We parse 'c1' -> cam_id=1\n",
    "    \"\"\"\n",
    "    m = re.match(r\"c(\\d+)\", stem)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Cannot parse cam_id from stem: {stem}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "\n",
    "def clip_box_xyxy(box: np.ndarray, w: int, h: int) -> np.ndarray:\n",
    "    x1, y1, x2, y2 = box\n",
    "    x1 = float(np.clip(x1, 0, w - 1))\n",
    "    y1 = float(np.clip(y1, 0, h - 1))\n",
    "    x2 = float(np.clip(x2, 0, w - 1))\n",
    "    y2 = float(np.clip(y2, 0, h - 1))\n",
    "    x1, x2 = min(x1, x2), max(x1, x2)\n",
    "    y1, y2 = min(y1, y2), max(y1, y2)\n",
    "    return np.array([x1, y1, x2, y2], dtype=np.float32)\n",
    "\n",
    "\n",
    "def expand_and_jitter_box_xyxy(\n",
    "    box: np.ndarray,\n",
    "    img_w: int,\n",
    "    img_h: int,\n",
    "    expand_ratio: float = 0.2,\n",
    "    jitter_ratio: float = 0.05,\n",
    "    do_jitter: bool = True,\n",
    ") -> np.ndarray:\n",
    "    x1, y1, x2, y2 = box.astype(np.float32)\n",
    "    bw, bh = (x2 - x1), (y2 - y1)\n",
    "    cx, cy = x1 + 0.5 * bw, y1 + 0.5 * bh\n",
    "\n",
    "    # expand\n",
    "    bw2 = bw * (1.0 + expand_ratio)\n",
    "    bh2 = bh * (1.0 + expand_ratio)\n",
    "\n",
    "    if do_jitter:\n",
    "        cx += (random.uniform(-1, 1) * jitter_ratio) * bw\n",
    "        cy += (random.uniform(-1, 1) * jitter_ratio) * bh\n",
    "        scale = 1.0 + random.uniform(-jitter_ratio, jitter_ratio)\n",
    "        bw2 *= scale\n",
    "        bh2 *= scale\n",
    "\n",
    "    nx1 = cx - 0.5 * bw2\n",
    "    ny1 = cy - 0.5 * bh2\n",
    "    nx2 = cx + 0.5 * bw2\n",
    "    ny2 = cy + 0.5 * bh2\n",
    "    return clip_box_xyxy(np.array([nx1, ny1, nx2, ny2], dtype=np.float32), img_w, img_h)\n",
    "\n",
    "\n",
    "def load_id_list(mat_path: Path, key: str) -> List[int]:\n",
    "    d = scipy.io.loadmat(mat_path)\n",
    "    if key not in d:\n",
    "        raise KeyError(f\"Key '{key}' not found in {mat_path}. Keys: {list(d.keys())}\")\n",
    "    arr = np.asarray(d[key]).reshape(-1)\n",
    "    return [int(x) for x in arr]\n",
    "\n",
    "\n",
    "class PRWReIDDatasetCE(Dataset):\n",
    "    \"\"\"\n",
    "    Crops GT boxes from the PRW_dataset and returns:\n",
    "      crop_tensor, label (0..C-1), pid, camid\n",
    "\n",
    "    CE needs contiguous labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prw_det_ds,                                # the PRW_dataset(split=\"train\" or \"test\")\n",
    "        crop_transform: Optional[Callable] = None,\n",
    "        id_train_mat: Optional[Path] = None,       # e.g. ID_train.mat (recommended for training)\n",
    "        id_test_mat: Optional[Path] = None,        # e.g. ID_test.mat (recommended for test eval)\n",
    "        split: str = \"train\",                      # \"train\" or \"test\" just for ID filtering\n",
    "        filter_invalid_ids: bool = True,           # ignore pid <= 0\n",
    "        min_box_size: int = 10,\n",
    "        expand_ratio: float = 0.2,\n",
    "        jitter_ratio: float = 0.05,\n",
    "        jitter: bool = True,\n",
    "    ):\n",
    "        self.base = prw_det_ds\n",
    "        self.crop_transform = crop_transform\n",
    "        self.filter_invalid_ids = filter_invalid_ids\n",
    "        self.min_box_size = min_box_size\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.jitter_ratio = jitter_ratio\n",
    "        self.jitter = jitter\n",
    "\n",
    "        split = split.lower()\n",
    "        assert split in {\"train\", \"test\"}\n",
    "        self.split = split\n",
    "\n",
    "        # Optional: restrict to train/test IDs based on provided mats\n",
    "        allowed_ids = None\n",
    "        if split == \"train\" and id_train_mat is not None:\n",
    "            # common key name in PRW releases is \"ID_train\"\n",
    "            allowed_ids = set(load_id_list(id_train_mat, \"ID_train\"))\n",
    "        if split == \"test\" and id_test_mat is not None:\n",
    "            allowed_ids = set(load_id_list(id_test_mat, \"ID_test2\"))\n",
    "\n",
    "        self.samples: List[Dict] = []\n",
    "        for base_idx, (img_path, ann_path) in enumerate(self.base.pairs):\n",
    "            boxes_np, ids_np = self.base.box_cache[str(ann_path)]\n",
    "            if boxes_np is None or len(boxes_np) == 0:\n",
    "                continue\n",
    "\n",
    "            if self.filter_invalid_ids:\n",
    "                keep = ids_np > 0\n",
    "                boxes_np = boxes_np[keep]\n",
    "                ids_np = ids_np[keep]\n",
    "\n",
    "            if allowed_ids is not None:\n",
    "                keep = np.array([pid in allowed_ids for pid in ids_np], dtype=bool)\n",
    "                boxes_np = boxes_np[keep]\n",
    "                ids_np = ids_np[keep]\n",
    "\n",
    "            stem = Path(img_path).stem\n",
    "            camid = parse_cam_id_from_stem(stem)\n",
    "\n",
    "            for b, pid in zip(boxes_np, ids_np):\n",
    "                x1, y1, x2, y2 = b.tolist()\n",
    "                if (x2 - x1) < self.min_box_size or (y2 - y1) < self.min_box_size:\n",
    "                    continue\n",
    "                self.samples.append(\n",
    "                    {\n",
    "                        \"img_path\": str(img_path),\n",
    "                        \"bbox\": b.astype(np.float32),\n",
    "                        \"pid\": int(pid),\n",
    "                        \"camid\": int(camid),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # CE needs contiguous labels\n",
    "        self.pids = sorted({s[\"pid\"] for s in self.samples})\n",
    "        self.pid2label = {pid: i for i, pid in enumerate(self.pids)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        s = self.samples[i]\n",
    "        img = Image.open(s[\"img_path\"]).convert(\"RGB\")\n",
    "        W, H = img.size\n",
    "\n",
    "        box = expand_and_jitter_box_xyxy(\n",
    "            s[\"bbox\"], W, H,\n",
    "            expand_ratio=self.expand_ratio,\n",
    "            jitter_ratio=self.jitter_ratio,\n",
    "            do_jitter=self.jitter,\n",
    "        )\n",
    "\n",
    "        x1, y1, x2, y2 = box\n",
    "        crop = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "        if self.crop_transform is not None:\n",
    "            crop = self.crop_transform(crop)\n",
    "        else:\n",
    "            crop = FT.to_tensor(crop)\n",
    "\n",
    "        pid = s[\"pid\"]\n",
    "        label = self.pid2label[pid]\n",
    "        camid = s[\"camid\"]\n",
    "\n",
    "        return crop, torch.tensor(label, dtype=torch.long), torch.tensor(pid), torch.tensor(camid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:20:09.028108Z",
     "iopub.status.busy": "2026-02-10T10:20:09.027621Z",
     "iopub.status.idle": "2026-02-10T10:20:09.033057Z",
     "shell.execute_reply": "2026-02-10T10:20:09.032461Z",
     "shell.execute_reply.started": "2026-02-10T10:20:09.028077Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Performing augmentation just on training bboxes\n",
    "train_reid_tf = T.Compose([\n",
    "    T.Resize((256, 128)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_reid_tf = T.Compose([\n",
    "    T.Resize((256, 128)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T11:57:27.148427Z",
     "iopub.status.busy": "2026-02-10T11:57:27.147868Z",
     "iopub.status.idle": "2026-02-10T11:57:27.292538Z",
     "shell.execute_reply": "2026-02-10T11:57:27.291977Z",
     "shell.execute_reply.started": "2026-02-10T11:57:27.148399Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train on GT bboxes\n",
    "train_reid_ds = PRWReIDDatasetCE(\n",
    "    train_view,\n",
    "    crop_transform=train_reid_tf,\n",
    "    split=\"train\",\n",
    "    filter_invalid_ids=True,\n",
    "    id_train_mat=None, \n",
    ")\n",
    "\n",
    "val_reid_ds = PRWReIDDatasetCE(\n",
    "    val_view,\n",
    "    crop_transform=test_reid_tf,\n",
    "    split=\"train\",\n",
    "    filter_invalid_ids=True,\n",
    "    id_train_mat=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T11:57:41.293364Z",
     "iopub.status.busy": "2026-02-10T11:57:41.292629Z",
     "iopub.status.idle": "2026-02-10T11:57:41.297311Z",
     "shell.execute_reply": "2026-02-10T11:57:41.296718Z",
     "shell.execute_reply.started": "2026-02-10T11:57:41.293335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "train_reid_loader = DataLoader(\n",
    "    train_reid_ds,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4,            \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,  # it makes sense only if num_workers>0\n",
    "    prefetch_factor=2         # default is 2\n",
    ")\n",
    "\n",
    "val_reid_loader = DataLoader(\n",
    "    val_reid_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,            \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,  # it makes sense only if num_workers>0\n",
    "    prefetch_factor=2         # default is 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:20:16.404108Z",
     "iopub.status.busy": "2026-02-10T10:20:16.403589Z",
     "iopub.status.idle": "2026-02-10T10:20:17.074099Z",
     "shell.execute_reply": "2026-02-10T10:20:17.073327Z",
     "shell.execute_reply.started": "2026-02-10T10:20:16.404081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import average_precision_score\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:20:21.654097Z",
     "iopub.status.busy": "2026-02-10T10:20:21.653332Z",
     "iopub.status.idle": "2026-02-10T10:20:21.673617Z",
     "shell.execute_reply": "2026-02-10T10:20:21.672741Z",
     "shell.execute_reply.started": "2026-02-10T10:20:21.654065Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This is a minimally modified version of the eval function from the SeqNet repository (https://github.com/serend1p1ty/SeqNet/blob/master/eval_func.py)\n",
    "# Changes:\n",
    "# - Removed code related to CBGM (Context Bipartite Graph Matching)\n",
    "# - Adjusted top-k accuracy calculation to only consider top-1 accuracy\n",
    "# - Clarified function docstring and added recall rate scaling explanation\n",
    "\n",
    "def _compute_iou(a, b):\n",
    "    x1 = max(a[0], b[0])\n",
    "    y1 = max(a[1], b[1])\n",
    "    x2 = min(a[2], b[2])\n",
    "    y2 = min(a[3], b[3])\n",
    "    inter = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    union = (a[2] - a[0]) * (a[3] - a[1]) + (b[2] - b[0]) * (b[3] - b[1]) - inter\n",
    "    return inter * 1.0 / union\n",
    "\n",
    "def eval_search_prw(\n",
    "    gallery_dataset,\n",
    "    query_dataset,\n",
    "    gallery_dets,\n",
    "    gallery_feats,\n",
    "    query_box_feats,\n",
    "    det_thresh,\n",
    "    ignore_cam_id=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate person search performance on PRW dataset.\n",
    "\n",
    "    Args:\n",
    "        gallery_dataset (Dataset): dataset containing gallery images.\n",
    "        query_dataset (Dataset): dataset containing query images.\n",
    "        gallery_dets (list of ndarray): n_det x [x1, x2, y1, y2, score] per image. \n",
    "        gallery_feats (list of ndarray): n_det x D features per image.\n",
    "        query_box_feats (list of ndarray): D dimensional features per query image.\n",
    "        det_thresh (float): filter out gallery detections whose scores below this.\n",
    "        ignore_cam_id (bool): whether to ignore camera ID during evaluation. If set to False,\n",
    "                            gallery images from the same camera as the query will be excluded. Default: True.\n",
    "    \"\"\"\n",
    "    assert len(gallery_dataset) == len(gallery_dets)\n",
    "    assert len(gallery_dataset) == len(gallery_feats)\n",
    "    assert len(query_dataset) == len(query_box_feats)\n",
    "\n",
    "    annos = gallery_dataset.annotations\n",
    "    name_to_det_feat = {}\n",
    "    for anno, det, feat in zip(annos, gallery_dets, gallery_feats):\n",
    "        name = anno[\"img_name\"]\n",
    "        scores = det[:, 4].ravel()\n",
    "        inds = np.where(scores >= det_thresh)[0]\n",
    "        if len(inds) > 0:\n",
    "            name_to_det_feat[name] = (det[inds], feat[inds])\n",
    "\n",
    "    aps = []\n",
    "    accs = []\n",
    "    topk = [1] # we are only interested in top-1 accuracy\n",
    "    ret = {\"image_root\": gallery_dataset.img_prefix, \"results\": []}\n",
    "    for i in range(len(query_dataset)):\n",
    "        y_true, y_score = [], []\n",
    "        imgs, rois = [], []\n",
    "        count_gt, count_tp = 0, 0\n",
    "\n",
    "        feat_p = query_box_feats[i].ravel()\n",
    "\n",
    "        query_imname = query_dataset.annotations[i][\"img_name\"]\n",
    "        query_roi = query_dataset.annotations[i][\"boxes\"]\n",
    "        query_pid = query_dataset.annotations[i][\"pids\"]\n",
    "        query_cam = query_dataset.annotations[i][\"cam_id\"]\n",
    "\n",
    "        # Find all occurence of this query\n",
    "        gallery_imgs = []\n",
    "        for x in annos:\n",
    "            if query_pid in x[\"pids\"] and x[\"img_name\"] != query_imname:\n",
    "                gallery_imgs.append(x)\n",
    "        query_gts = {}\n",
    "        for item in gallery_imgs:\n",
    "            query_gts[item[\"img_name\"]] = item[\"boxes\"][item[\"pids\"] == query_pid]\n",
    "\n",
    "        # Construct gallery set for this query\n",
    "        if ignore_cam_id:\n",
    "            gallery_imgs = []\n",
    "            for x in annos:\n",
    "                if x[\"img_name\"] != query_imname:\n",
    "                    gallery_imgs.append(x)\n",
    "        else:\n",
    "            gallery_imgs = []\n",
    "            for x in annos:\n",
    "                if x[\"img_name\"] != query_imname and x[\"cam_id\"] != query_cam:\n",
    "                    gallery_imgs.append(x)\n",
    "\n",
    "        name2sim = {}\n",
    "        sims = []\n",
    "        # 1. Go through all gallery samples\n",
    "        for item in gallery_imgs:\n",
    "            gallery_imname = item[\"img_name\"]\n",
    "            # some contain the query (gt not empty), some not\n",
    "            count_gt += gallery_imname in query_gts\n",
    "            # compute distance between query and gallery dets\n",
    "            if gallery_imname not in name_to_det_feat:\n",
    "                continue\n",
    "            det, feat_g = name_to_det_feat[gallery_imname]\n",
    "            # get L2-normalized feature matrix NxD\n",
    "    \n",
    "            feat_g = np.asarray(feat_g).reshape(len(det), -1)   # (Nd, D)\n",
    "            feat_p = np.asarray(feat_p).reshape(-1)            # (D,)\n",
    "            # compute cosine similarities\n",
    "            sim = feat_g.dot(feat_p).ravel()\n",
    "\n",
    "            if gallery_imname in name2sim:\n",
    "                continue\n",
    "            name2sim[gallery_imname] = sim\n",
    "            sims.extend(list(sim))\n",
    "\n",
    "        for gallery_imname, sim in name2sim.items():\n",
    "            det, feat_g = name_to_det_feat[gallery_imname]\n",
    "            # assign label for each det\n",
    "            label = np.zeros(len(sim), dtype=np.int32)\n",
    "            if gallery_imname in query_gts:\n",
    "                gt = query_gts[gallery_imname].ravel()\n",
    "                w, h = gt[2] - gt[0], gt[3] - gt[1]\n",
    "                iou_thresh = min(0.5, (w * h * 1.0) / ((w + 10) * (h + 10)))\n",
    "                inds = np.argsort(sim)[::-1]\n",
    "                sim = sim[inds]\n",
    "                det = det[inds]\n",
    "                # only set the first matched det as true positive\n",
    "                for j, roi in enumerate(det[:, :4]):\n",
    "                    if _compute_iou(roi, gt) >= iou_thresh:\n",
    "                        label[j] = 1\n",
    "                        count_tp += 1\n",
    "                        break\n",
    "            y_true.extend(list(label))\n",
    "            y_score.extend(list(sim))\n",
    "            imgs.extend([gallery_imname] * len(sim))\n",
    "            rois.extend(list(det))\n",
    "\n",
    "        # 2. Compute AP for this query (need to scale by recall rate)\n",
    "        y_score = np.asarray(y_score)\n",
    "        y_true = np.asarray(y_true)\n",
    "        assert count_tp <= count_gt\n",
    "        # Important: at the pedestrian detection stage, the model might have missed the person (failed to detect a box with IoU > 0.5).\n",
    "        # To penalize the model for these False Negatives at the detection stage, scale the AP by recall (the ratio of found matches to total ground truth matches). \n",
    "        # E.g. if the detector missed the person entirely 50% of the time, the final AP score is cut in half.\n",
    "        recall_rate = 0.0 if count_gt == 0 else (count_tp * 1.0 / count_gt)\n",
    "        ap = 0 if count_tp == 0 else average_precision_score(y_true, y_score) * recall_rate\n",
    "        #ap = 0 if count_tp == 0 else average_precision_score(y_true, y_score) # easier case\n",
    "\n",
    "        \n",
    "        aps.append(ap)\n",
    "        inds = np.argsort(y_score)[::-1]\n",
    "        y_score = y_score[inds]\n",
    "        y_true = y_true[inds]\n",
    "        accs.append([min(1, sum(y_true[:k])) for k in topk])\n",
    "        # 4. Save result for JSON dump\n",
    "        new_entry = {\n",
    "            \"query_img\": str(query_imname),\n",
    "            \"query_roi\": list(map(float, list(query_roi.squeeze()))),\n",
    "            \"query_gt\": query_gts,\n",
    "            \"gallery\": [],\n",
    "        }\n",
    "        # only save top-10 predictions\n",
    "        for k in range(10):\n",
    "            new_entry[\"gallery\"].append(\n",
    "                {\n",
    "                    \"img\": str(imgs[inds[k]]),\n",
    "                    \"roi\": list(map(float, list(rois[inds[k]]))),\n",
    "                    \"score\": float(y_score[k]),\n",
    "                    \"correct\": int(y_true[k]),\n",
    "                }\n",
    "            )\n",
    "        ret[\"results\"].append(new_entry)\n",
    "\n",
    "    print(\"search ranking:\")\n",
    "    mAP = np.mean(aps)\n",
    "    print(\"  mAP = {:.2%}\".format(mAP))\n",
    "    accs = np.mean(accs, axis=0)\n",
    "    for i, k in enumerate(topk):\n",
    "        print(\"  top-{:2d} = {:.2%}\".format(k, accs[i]))\n",
    "\n",
    "    # write_json(ret, \"vis/results.json\")\n",
    "\n",
    "    ret[\"mAP\"] = np.mean(aps)\n",
    "    ret[\"accs\"] = accs\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:20:28.458014Z",
     "iopub.status.busy": "2026-02-10T10:20:28.457707Z",
     "iopub.status.idle": "2026-02-10T10:20:28.465831Z",
     "shell.execute_reply": "2026-02-10T10:20:28.465106Z",
     "shell.execute_reply.started": "2026-02-10T10:20:28.457986Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_prw_query_info(query_info_path):\n",
    "    queries = []\n",
    "    with open(query_info_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            pid_s, x_s, y_s, w_s, h_s, stem = line.split()\n",
    "            pid = int(pid_s)\n",
    "            x, y, w, h = map(float, (x_s, y_s, w_s, h_s))\n",
    "            x1, y1, x2, y2 = x, y, x + w, y + h\n",
    "            queries.append({\n",
    "                \"pid\": pid,\n",
    "                \"stem\": stem,  # c1s3_016471\n",
    "                \"img_name\": stem + \".jpg\",\n",
    "                \"box_xyxy\": np.array([x1, y1, x2, y2], dtype=np.float32),\n",
    "                \"cam_id\": parse_cam_id_from_stem(stem),\n",
    "            })\n",
    "    return queries\n",
    "\n",
    "@dataclass\n",
    "class QueryDataset:\n",
    "    annotations: list\n",
    "    img_prefix: str = \"\"\n",
    "    def __len__(self): return len(self.annotations)\n",
    "\n",
    "def build_query_dataset_from_query_info(query_info_path):\n",
    "    queries = load_prw_query_info(query_info_path)\n",
    "    ann = []\n",
    "    for q in queries:\n",
    "        ann.append({\n",
    "            \"img_name\": q[\"img_name\"],\n",
    "            \"boxes\": q[\"box_xyxy\"][None, :],\n",
    "            \"pids\": np.array([q[\"pid\"]], dtype=np.int32),\n",
    "            \"cam_id\": int(q[\"cam_id\"]),\n",
    "        })\n",
    "    return QueryDataset(annotations=ann, img_prefix=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:20:32.334303Z",
     "iopub.status.busy": "2026-02-10T10:20:32.333468Z",
     "iopub.status.idle": "2026-02-10T10:20:32.340493Z",
     "shell.execute_reply": "2026-02-10T10:20:32.339751Z",
     "shell.execute_reply.started": "2026-02-10T10:20:32.334262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def forward_embed(model, batch):\n",
    "    'Call this to ignore logits but consider the embedding.'\n",
    "    out = model(batch)\n",
    "    if isinstance(out, (tuple, list)) and len(out) == 2:\n",
    "        logits, emb = out\n",
    "        return emb              # <-- 512\n",
    "    return out                  # fallback: model returns embedding directly\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_query_box_feats_from_querybox(\n",
    "    reid_model,\n",
    "    query_info_path,\n",
    "    query_box_dir,\n",
    "    transform,\n",
    "    device,\n",
    "):\n",
    "    reid_model.eval()\n",
    "    queries = load_prw_query_info(query_info_path)\n",
    "\n",
    "    feats = []\n",
    "    for q in queries:\n",
    "        pid = q[\"pid\"]\n",
    "        stem = q[\"stem\"]\n",
    "        path = osp.join(query_box_dir, f\"{pid}_{stem}.jpg\")\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "        emb = forward_embed(reid_model, x)             # <-- 512\n",
    "        emb = F.normalize(emb, p=2, dim=1)             # useful as sanity check\n",
    "        feats.append(emb.squeeze(0).detach().cpu().numpy().astype(np.float32))\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:20:36.435417Z",
     "iopub.status.busy": "2026-02-10T10:20:36.434723Z",
     "iopub.status.idle": "2026-02-10T10:20:36.443322Z",
     "shell.execute_reply": "2026-02-10T10:20:36.442627Z",
     "shell.execute_reply.started": "2026-02-10T10:20:36.435385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def build_gallery_feats_from_dets_prw_dataset(\n",
    "    reid_model,\n",
    "    prw_dataset,\n",
    "    detections,\n",
    "    transform,\n",
    "    device,\n",
    "):\n",
    "    reid_model.eval()\n",
    "    gallery_feats = []\n",
    "    assert len(prw_dataset) == len(detections)\n",
    "\n",
    "    for idx in range(len(prw_dataset)):\n",
    "        img_path, _ = prw_dataset.pairs[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        det = detections[idx]\n",
    "        if det.shape[0] == 0:\n",
    "            gallery_feats.append(np.zeros((0, 1), dtype=np.float32))\n",
    "            continue\n",
    "\n",
    "        feats_img = []\n",
    "        for box in det:\n",
    "            x1, y1, x2, y2 = box[:4]\n",
    "            x1, y1, x2, y2 = map(int, map(round, (x1, y1, x2, y2)))\n",
    "            x1, y1 = max(0, x1), max(0, y1)\n",
    "            x2, y2 = max(x1 + 1, x2), max(y1 + 1, y2)\n",
    "\n",
    "            crop = img.crop((x1, y1, x2, y2))\n",
    "            x = transform(crop).unsqueeze(0).to(device)\n",
    "\n",
    "            emb = forward_embed(reid_model, x)         # <-- 512\n",
    "            emb = F.normalize(emb, p=2, dim=1)         # useful for sanity check\n",
    "            feats_img.append(emb.squeeze(0).detach().cpu().numpy())\n",
    "\n",
    "        gallery_feats.append(np.vstack(feats_img).astype(np.float32))\n",
    "\n",
    "    # fix empty dim\n",
    "    D = None\n",
    "    for f in gallery_feats:\n",
    "        if f.shape[0] > 0:\n",
    "            D = f.shape[1]\n",
    "            break\n",
    "    if D is None:\n",
    "        raise ValueError(\"All gallery feats are empty (no detections?).\")\n",
    "    for i, f in enumerate(gallery_feats):\n",
    "        if f.shape[0] == 0 and f.shape[1] != D:\n",
    "            gallery_feats[i] = np.zeros((0, D), dtype=np.float32)\n",
    "\n",
    "    return gallery_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:20:40.522596Z",
     "iopub.status.busy": "2026-02-10T10:20:40.522284Z",
     "iopub.status.idle": "2026-02-10T10:20:40.530769Z",
     "shell.execute_reply": "2026-02-10T10:20:40.529929Z",
     "shell.execute_reply.started": "2026-02-10T10:20:40.522568Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PRWGalleryForEval:\n",
    "    annotations: list\n",
    "    img_prefix: str\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "def build_gallery_eval_view(prw_dataset):\n",
    "    \"\"\"\n",
    "    Create a lightweight object with .annotations and .img_prefix\n",
    "    compatible with eval_search_prw.\n",
    "    \"\"\"\n",
    "    annos = []\n",
    "    # img_prefix: parent dir of frames\n",
    "    # pairs[idx][0] is .../frames/c1s3_016471.jpg\n",
    "    # so prefix is .../frames\n",
    "    img_prefix = str(prw_dataset.pairs[0][0].parent)\n",
    "\n",
    "    for idx, (img_path, ann_path) in enumerate(prw_dataset.pairs):\n",
    "        stem = img_path.stem              # c1s3_016471\n",
    "        img_name = img_path.name          # c1s3_016471.jpg\n",
    "\n",
    "        boxes_np, ids_np = prw_dataset.box_cache[str(ann_path)]  # GT xyxy + ids\n",
    "\n",
    "        keep = ids_np > 0\n",
    "        boxes = boxes_np[keep]\n",
    "        pids = ids_np[keep]\n",
    "\n",
    "        cam_id = parse_cam_id_from_stem(stem)\n",
    "\n",
    "        annos.append({\n",
    "            \"img_name\": img_name,  # must match exactly what eval expects\n",
    "            \"boxes\": boxes.astype(np.float32),\n",
    "            \"pids\": pids.astype(np.int32),\n",
    "            \"cam_id\": cam_id,\n",
    "        })\n",
    "\n",
    "    return PRWGalleryForEval(annotations=annos, img_prefix=img_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:20:44.454354Z",
     "iopub.status.busy": "2026-02-10T10:20:44.453762Z",
     "iopub.status.idle": "2026-02-10T10:20:44.463750Z",
     "shell.execute_reply": "2026-02-10T10:20:44.462916Z",
     "shell.execute_reply.started": "2026-02-10T10:20:44.454324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_checkpoint_prw(\n",
    "    ckpt_path,\n",
    "    model,\n",
    "    query_ds,\n",
    "    gallery_eval,\n",
    "    test_ds,\n",
    "    test_detections,\n",
    "    test_reid_tf,\n",
    "    device,\n",
    "):\n",
    "    def _fmt_pct(x):\n",
    "        x = float(x)\n",
    "        return f\"{x:.2f}%\" if x > 1.0 else f\"{100*x:.2f}%\"\n",
    "\n",
    "    def _extract_map_top1(ret):\n",
    "        # --- mAP ---\n",
    "        map_val = ret.get(\"mAP\", ret.get(\"map\", ret.get(\"MAP\", None)))\n",
    "\n",
    "        # --- top-1 (rank-1) ---\n",
    "        top1_val = None\n",
    "        accs = ret.get(\"accs\", None)\n",
    "\n",
    "        if isinstance(accs, (list, tuple)):\n",
    "            if len(accs) > 0:\n",
    "                top1_val = accs[0]\n",
    "        elif hasattr(accs, \"shape\"):  # numpy array / torch tensor\n",
    "            try:\n",
    "                # numpy\n",
    "                if getattr(accs, \"size\", 0) > 0:\n",
    "                    top1_val = accs[0]\n",
    "            except Exception:\n",
    "                # torch tensor fallback\n",
    "                if accs.numel() > 0:\n",
    "                    top1_val = accs.flatten()[0].item()\n",
    "        elif isinstance(accs, dict):\n",
    "            for k in [\"top1\", \"top-1\", \"rank1\", \"r1\", \"acc1\"]:\n",
    "                if k in accs:\n",
    "                    top1_val = accs[k]\n",
    "                    break\n",
    "\n",
    "        return map_val, top1_val\n",
    "\n",
    "    print(f\"\\n[Eval] Loading checkpoint: {ckpt_path}\")\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    state_dict = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else ckpt\n",
    "\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    query_feats = compute_query_box_feats_from_querybox(\n",
    "        reid_model=model,\n",
    "        query_info_path=\"/kaggle/input/prw-person-re-identification-in-the-wild/query_info.txt\",\n",
    "        query_box_dir=\"/kaggle/input/prw-person-re-identification-in-the-wild/query_box\",\n",
    "        transform=test_reid_tf,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    gallery_feats = build_gallery_feats_from_dets_prw_dataset(\n",
    "        reid_model=model,\n",
    "        prw_dataset=test_ds,\n",
    "        detections=test_detections,\n",
    "        transform=test_reid_tf,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    ret = eval_search_prw(\n",
    "        gallery_dataset=gallery_eval,\n",
    "        query_dataset=query_ds,\n",
    "        gallery_dets=test_detections,\n",
    "        gallery_feats=gallery_feats,\n",
    "        query_box_feats=query_feats,\n",
    "        det_thresh=0.3,\n",
    "        ignore_cam_id=True,\n",
    "    )\n",
    "\n",
    "    map_val, top1_val = _extract_map_top1(ret)\n",
    "\n",
    "    if map_val is not None and top1_val is not None:\n",
    "        print(f\"[Eval] {ckpt_path}  mAP={_fmt_pct(map_val)} | top-1={_fmt_pct(top1_val)}\")\n",
    "    else:\n",
    "        print(f\"[Eval] {ckpt_path}  cannot find mAP/top-1 in ret. keys={list(ret.keys())}\")\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T11:58:08.668951Z",
     "iopub.status.busy": "2026-02-10T11:58:08.668654Z",
     "iopub.status.idle": "2026-02-10T11:58:08.749587Z",
     "shell.execute_reply": "2026-02-10T11:58:08.748985Z",
     "shell.execute_reply.started": "2026-02-10T11:58:08.668925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1) Build gallery view for eval (GT annos etc.)\n",
    "gallery_eval = build_gallery_eval_view(test_ds)\n",
    "\n",
    "# 2) Build query dataset from query_info (metadata only)\n",
    "query_ds = build_query_dataset_from_query_info(\n",
    "    \"/kaggle/input/prw-person-re-identification-in-the-wild/query_info.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T11:58:13.323380Z",
     "iopub.status.busy": "2026-02-10T11:58:13.322781Z",
     "iopub.status.idle": "2026-02-10T11:58:13.327639Z",
     "shell.execute_reply": "2026-02-10T11:58:13.326881Z",
     "shell.execute_reply.started": "2026-02-10T11:58:13.323351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Defining hyperparams\n",
    "emb_dim = 512\n",
    "n_epochs = 20\n",
    "num_classes = len(train_reid_ds.pids)\n",
    "\n",
    "# optimizer params\n",
    "lr = 3e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# setup device \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# setup training\n",
    "use_amp = (device == \"cuda\")          \n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=use_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T09:45:39.018314Z",
     "iopub.status.busy": "2026-02-08T09:45:39.018011Z",
     "iopub.status.idle": "2026-02-08T09:45:39.023189Z",
     "shell.execute_reply": "2026-02-08T09:45:39.022603Z",
     "shell.execute_reply.started": "2026-02-08T09:45:39.018288Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "len(train_reid_ds.pids) # num. train samples for re-id (331 IDs for 85% view (originally 483))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory runs (considering test as val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Softmax C.E. with Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Cosine similarity to compare embeddings!\n",
    "They will be L2-normalized and we will use a C.E. loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:21:14.502123Z",
     "iopub.status.busy": "2026-02-10T10:21:14.501822Z",
     "iopub.status.idle": "2026-02-10T10:21:14.507270Z",
     "shell.execute_reply": "2026-02-10T10:21:14.506636Z",
     "shell.execute_reply.started": "2026-02-10T10:21:14.502095Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CosineClassifier(nn.Module):\n",
    "    def __init__(self, in_dim: int, num_classes: int, s: float = 30.0):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(num_classes, in_dim))\n",
    "        nn.init.normal_(self.weight, std=0.01)\n",
    "        self.s = s\n",
    "\n",
    "    def forward(self, x_normed: torch.Tensor, labels=None) -> torch.Tensor:\n",
    "        # x_normed: already L2-normalized features (B, D)\n",
    "        w = F.normalize(self.weight, dim=1)           # (C, D)\n",
    "        logits = x_normed @ w.t()                     # cosine similarity\n",
    "        return self.s * logits                        # scale for CE stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:21:17.895594Z",
     "iopub.status.busy": "2026-02-10T10:21:17.895032Z",
     "iopub.status.idle": "2026-02-10T10:21:17.900784Z",
     "shell.execute_reply": "2026-02-10T10:21:17.900207Z",
     "shell.execute_reply.started": "2026-02-10T10:21:17.895562Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ReIDNet(nn.Module):\n",
    "    def __init__(self, emb_dim: int, num_classes: int, s: float = 30.0):\n",
    "        super().__init__()\n",
    "        backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-2])  # conv until layer4\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(2048, emb_dim)\n",
    "        self.cls = CosineClassifier(emb_dim, num_classes, s=s)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        f = self.backbone(x)                   # (B,2048,H',W')\n",
    "        f = self.pool(f).flatten(1)            # (B,2048)\n",
    "        z = self.fc(f)                         # (B,emb_dim)\n",
    "        z = F.normalize(z, dim=1)              # normalized embeddings\n",
    "        logits = self.cls(z, labels=labels)    # scaled cosine logits\n",
    "        return logits, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T10:44:46.985506Z",
     "iopub.status.busy": "2026-01-29T10:44:46.985139Z",
     "iopub.status.idle": "2026-01-29T12:07:54.397836Z",
     "shell.execute_reply": "2026-01-29T12:07:54.397226Z",
     "shell.execute_reply.started": "2026-01-29T10:44:46.985477Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/4266526679.py:32: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>step</td><td></td></tr><tr><td>train/acc1</td><td></td></tr><tr><td>train/emb_norm_mean</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/lr</td><td></td></tr><tr><td>train/max_prob_mean</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>step</td><td>120</td></tr><tr><td>train/acc1</td><td>0.14062</td></tr><tr><td>train/emb_norm_mean</td><td>1</td></tr><tr><td>train/loss</td><td>5.04541</td></tr><tr><td>train/lr</td><td>8e-05</td></tr><tr><td>train/max_prob_mean</td><td>0.09338</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">reid_r50_ce_20e_adamw_amp_1769683225</strong> at: <a href='https://wandb.ai/unibo-ai/person%20re-id/runs/wn1vhv31' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id/runs/wn1vhv31</a><br> View project at: <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260129_104033-wn1vhv31/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20260129_104447-fl5tz2j3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unibo-ai/person%20re-id/runs/fl5tz2j3' target=\"_blank\">reid_r50_ce_20e_adamw_warmcos_amp_1769683487</a></strong> to <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unibo-ai/person%20re-id/runs/fl5tz2j3' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id/runs/fl5tz2j3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] loss=5.1215 acc1=0.1592\n",
      "[Epoch 1] loss=1.1890 acc1=0.7288\n",
      "[Epoch 2] loss=0.2905 acc1=0.9298\n",
      "[Epoch 3] loss=0.1124 acc1=0.9738\n",
      "[Epoch 4] loss=0.0599 acc1=0.9889\n",
      "[Epoch 5] loss=0.0375 acc1=0.9934\n",
      "[Epoch 6] loss=0.0255 acc1=0.9958\n",
      "[Epoch 7] loss=0.0147 acc1=0.9977\n",
      "[Epoch 8] loss=0.0178 acc1=0.9972\n",
      "[Epoch 9] loss=0.0105 acc1=0.9985\n",
      "[Epoch 10] loss=0.0093 acc1=0.9989\n",
      "[Epoch 11] loss=0.0061 acc1=0.9991\n",
      "[Epoch 12] loss=0.0040 acc1=0.9997\n",
      "[Epoch 13] loss=0.0028 acc1=0.9997\n",
      "[Epoch 14] loss=0.0033 acc1=0.9996\n",
      "[Epoch 15] loss=0.0017 acc1=1.0000\n",
      "[Epoch 16] loss=0.0025 acc1=0.9997\n",
      "[Epoch 17] loss=0.0019 acc1=0.9998\n",
      "[Epoch 18] loss=0.0017 acc1=0.9999\n",
      "[Epoch 19] loss=0.0015 acc1=0.9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Artifact reid_20e_ckpt>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------- MODEL --------------------\n",
    "model_reid = ReIDNet(emb_dim=emb_dim, num_classes=num_classes).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# -------------------- OPTIMIZER (AdamW) --------------------\n",
    "optimizer = torch.optim.AdamW(model_reid.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# -------------------- SCHEDULER: warmup + cosine (per-iter) --------------------\n",
    "steps_per_epoch = len(train_reid_loader)\n",
    "total_steps = n_epochs * steps_per_epoch\n",
    "warmup_steps = int(0.1 * total_steps)     # 10% warmup\n",
    "min_lr_ratio = 0.01                       # lr finale = lr * 0.01\n",
    "\n",
    "def lr_lambda(step: int) -> float:\n",
    "    if step < warmup_steps:\n",
    "        return (step + 1) / max(1, warmup_steps)  # linear warmup 0->1\n",
    "    t = (step - warmup_steps) / max(1, total_steps - warmup_steps)  # 0..1\n",
    "    cosine = 0.5 * (1.0 + math.cos(math.pi * t))                    # 1..0\n",
    "    return min_lr_ratio + (1.0 - min_lr_ratio) * cosine             # 1..min_lr_ratio\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# -------------------- WANDB INIT --------------------\n",
    "run = wandb.init(\n",
    "    entity=\"unibo-ai\",\n",
    "    project=\"person re-id\",\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"architecture\": \"ReIDNet\",\n",
    "        \"dataset\": \"PRW\",\n",
    "        \"epochs\": n_epochs,\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"batch_size\": getattr(train_reid_loader, \"batch_size\", None),\n",
    "        \"sampler\": type(getattr(train_reid_loader, \"batch_sampler\", None)).__name__\n",
    "                  if getattr(train_reid_loader, \"batch_sampler\", None) is not None else \"shuffle\",\n",
    "        \"resize\": \"(256,128)\",\n",
    "        \"loss\": \"CE\",\n",
    "        \"amp\": use_amp,\n",
    "        \"scheduler\": \"warmup+cosine (LambdaLR, per-iter)\",\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"min_lr_ratio\": min_lr_ratio,\n",
    "    },\n",
    "    name=f\"reid_r50_ce_20e_adamw_warmcos_amp_{int(time.time())}\",\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "# -------------------- TRAIN LOOP --------------------\n",
    "global_step = 0\n",
    "for epoch in range(n_epochs):\n",
    "    model_reid.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc1 = 0.0\n",
    "\n",
    "    for crops, labels, pid, camid in train_reid_loader:\n",
    "        crops  = crops.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True).view(-1)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # forward + loss in autocast\n",
    "        with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "            logits, emb = model_reid(crops)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        # backward + optimizer step with GradScaler\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer) \n",
    "        scaler.update()\n",
    "\n",
    "        # scheduler per-iter AFTER optimizer step\n",
    "        scheduler.step()\n",
    "\n",
    "        # -------------------- METRICS --------------------\n",
    "        with torch.no_grad():\n",
    "            loss_val = float(loss.item())\n",
    "            pred = logits.argmax(dim=1)\n",
    "            acc1 = (pred == labels).float().mean().item()\n",
    "            emb_norm = emb.norm(dim=1).mean().item()\n",
    "            max_prob = F.softmax(logits, dim=1).max(dim=1).values.mean().item()\n",
    "\n",
    "        running_loss += loss_val\n",
    "        running_acc1 += acc1\n",
    "\n",
    "        # -------------------- WANDB LOG (per-step) --------------------\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train/loss\": loss_val,\n",
    "                \"train/acc1\": acc1,\n",
    "                \"train/emb_norm_mean\": emb_norm,\n",
    "                \"train/max_prob_mean\": max_prob,\n",
    "                \"train/lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                \"epoch\": epoch,\n",
    "                \"step\": global_step,\n",
    "            },\n",
    "            step=global_step\n",
    "        )\n",
    "        global_step += 1\n",
    "\n",
    "    # -------------------- EPOCH LOG --------------------\n",
    "    epoch_loss = running_loss / max(1, len(train_reid_loader))\n",
    "    epoch_acc1 = running_acc1 / max(1, len(train_reid_loader))\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train/epoch_loss_avg\": epoch_loss,\n",
    "            \"train/epoch_acc1_avg\": epoch_acc1,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "        step=global_step\n",
    "    )\n",
    "    print(f\"[Epoch {epoch}] loss={epoch_loss:.4f} acc1={epoch_acc1:.4f}\")\n",
    "\n",
    "# -------------------- SAVE CHECKPOINT --------------------\n",
    "ckpt = {\n",
    "    \"model\": model_reid.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    \"scheduler\": scheduler.state_dict(),\n",
    "    \"scaler\": scaler.state_dict() if use_amp else None,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"emb_dim\": emb_dim,\n",
    "    \"pid2label\": train_reid_ds.pid2label,\n",
    "    \"config\": {\n",
    "        \"lr\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"warmup_ratio\": 0.1,\n",
    "        \"min_lr_ratio\": min_lr_ratio,\n",
    "        \"use_amp\": use_amp,\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(ckpt, \"reid_20e_ckpt.pth\")\n",
    "\n",
    "artifact = wandb.Artifact(\"reid_20e_ckpt\", type=\"model\")\n",
    "artifact.add_file(\"reid_20e_ckpt.pth\")\n",
    "wandb.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading re-id model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T21:49:07.761002Z",
     "iopub.status.busy": "2026-01-25T21:49:07.760467Z",
     "iopub.status.idle": "2026-01-25T21:49:14.372813Z",
     "shell.execute_reply": "2026-01-25T21:49:14.372004Z",
     "shell.execute_reply.started": "2026-01-25T21:49:07.760971Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 97.8M/97.8M [00:00<00:00, 177MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ReIDNet(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (fc): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (cls): CosineClassifier()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load weights\n",
    "weights_reid_path = \"/kaggle/input/reid-weights-5-epochs/reid_ckpt.pth\"\n",
    "ckpt_reid = torch.load(weights_reid_path, map_location=\"cpu\")\n",
    "\n",
    "# instantiate model\n",
    "model_reid = ReIDNet(\n",
    "    emb_dim=ckpt_reid[\"emb_dim\"],\n",
    "    num_classes=ckpt_reid[\"num_classes\"]\n",
    ")\n",
    "\n",
    "# load weights into model\n",
    "model_reid.load_state_dict(ckpt_reid[\"model\"], strict=True)\n",
    "model_reid.to(device)\n",
    "model_reid.eval()   # inference mode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation with metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T23:25:55.455047Z",
     "iopub.status.busy": "2026-01-25T23:25:55.454755Z",
     "iopub.status.idle": "2026-01-25T23:30:07.226680Z",
     "shell.execute_reply": "2026-01-25T23:30:07.225655Z",
     "shell.execute_reply.started": "2026-01-25T23:25:55.455023Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search ranking:\n",
      "  mAP = 41.39%\n",
      "  top- 1 = 83.81%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation: 5 epochs model\n",
    "ret = eval_search_prw(\n",
    "    gallery_dataset=gallery_eval,\n",
    "    query_dataset=query_ds,\n",
    "    gallery_dets=test_detections,\n",
    "    gallery_feats=test_gallery_feats,\n",
    "    query_box_feats=query_feats,\n",
    "    det_thresh=0.3,\n",
    "    ignore_cam_id=True, # simplyfing the problem: using same camera\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T12:11:48.471361Z",
     "iopub.status.busy": "2026-01-29T12:11:48.470776Z",
     "iopub.status.idle": "2026-01-29T12:18:14.965789Z",
     "shell.execute_reply": "2026-01-29T12:18:14.965161Z",
     "shell.execute_reply.started": "2026-01-29T12:11:48.471323Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query dim: (512,)\n",
      "gallery dim: (4, 512)\n",
      "{512}\n"
     ]
    }
   ],
   "source": [
    "# Compute query features from query_box (NO crop-from-frame)\n",
    "query_feats = compute_query_box_feats_from_querybox(\n",
    "    reid_model=model_reid,\n",
    "    query_info_path=\"/kaggle/input/prw-person-re-identification-in-the-wild/query_info.txt\",\n",
    "    query_box_dir=\"/kaggle/input/prw-person-re-identification-in-the-wild/query_box\",\n",
    "    transform=test_reid_tf,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Compute gallery features aligned with detector outputs\n",
    "test_gallery_feats = build_gallery_feats_from_dets_prw_dataset(\n",
    "    reid_model=model_reid,\n",
    "    prw_dataset=test_ds,\n",
    "    detections=test_detections,\n",
    "    transform=test_reid_tf,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"query dim:\", query_feats[0].shape)              # (512,)\n",
    "print(\"gallery dim:\", test_gallery_feats[0].shape)          # (N,512)\n",
    "print({f.shape[1] for f in test_gallery_feats if f.shape[0] > 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The obtained results are pretty good, compared to the ones in literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-25T23:34:19.761521Z",
     "iopub.status.busy": "2026-01-25T23:34:19.760785Z",
     "iopub.status.idle": "2026-01-25T23:38:00.272662Z",
     "shell.execute_reply": "2026-01-25T23:38:00.271744Z",
     "shell.execute_reply.started": "2026-01-25T23:34:19.761487Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search ranking:\n",
      "  mAP = 37.87%\n",
      "  top- 1 = 65.34%\n"
     ]
    }
   ],
   "source": [
    "# ignore same camera imgs: 5 epochs model\n",
    "ret = eval_search_prw(\n",
    "    gallery_dataset=gallery_eval,\n",
    "    query_dataset=query_ds,\n",
    "    gallery_dets=test_detections,\n",
    "    gallery_feats=test_gallery_feats,\n",
    "    query_box_feats=query_feats,\n",
    "    det_thresh=0.3,\n",
    "    ignore_cam_id=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The results with ignore_cam_id = False are worse because we are actually ignoring (excluding) gallery images from the same camera of the query. This means that we are removing **easy matches**, because images from the same camera have:\n",
    "> * same illumination conditions;\n",
    "> * same viewpoint and perspective;\n",
    "> * same scale;\n",
    "> * same background context;\n",
    "> If ignore_cam_id was set equal to True, the resulting embeddings would have been called **shortcut features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T12:25:14.533565Z",
     "iopub.status.busy": "2026-01-29T12:25:14.532883Z",
     "iopub.status.idle": "2026-01-29T12:28:20.938471Z",
     "shell.execute_reply": "2026-01-29T12:28:20.937479Z",
     "shell.execute_reply.started": "2026-01-29T12:25:14.533529Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search ranking:\n",
      "  mAP = 42.25%\n",
      "  top- 1 = 83.86%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation: 20 epochs model\n",
    "ret_20e = eval_search_prw(\n",
    "        gallery_dataset=gallery_eval,\n",
    "        query_dataset=query_ds,\n",
    "        gallery_dets=test_detections,\n",
    "        gallery_feats=test_gallery_feats,\n",
    "        query_box_feats=query_feats,\n",
    "        det_thresh=0.3,\n",
    "        ignore_cam_id=True, # simplyfing the problem: using same camera\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We gained ~0.86% for mAP and the ~0.05% on the top-1 metric!\n",
    "> It is a marginal improvement but still valuable and noteworthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First attempt to improve performance\n",
    "Up to now, we have achieved satisfying results with our baseline. The next natural question is: what can we improve to push performance further?\n",
    "\n",
    "The most straightforward modification is to change the **loss** function. So far, we trained the Re-ID network by L2-normalizing the embeddings and optimizing a softmax cross-entropy loss, which encourages class separability but does not explicitly enforce compactness of embeddings belonging to the same identity.\n",
    "\n",
    "To address this limitation, we introduce an angular **margin**, which explicitly enforces intra-class compactness and inter-class separation in the embedding space. This leads to margin-based losses such as **ArcFace**.\n",
    "\n",
    "Before adopting ArcFace directly, we first experiment with its simpler variant, **CosFace**, which approximates ArcFace by subtracting a margin  from the target class cosine similarity.\n",
    "\n",
    "The key difference between the two lies in how the margin is applied:\n",
    "\n",
    "* CosFace introduces an additive margin in the **cosine** similarity **space**;\n",
    "\n",
    "* ArcFace enforces a margin **directly** in the **angular space**, resulting in a constant angular separation on the hypersphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CosFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:21:34.105785Z",
     "iopub.status.busy": "2026-02-10T10:21:34.105034Z",
     "iopub.status.idle": "2026-02-10T10:21:34.113205Z",
     "shell.execute_reply": "2026-02-10T10:21:34.112455Z",
     "shell.execute_reply.started": "2026-02-10T10:21:34.105751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CosFaceClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, num_classes, s=30.0, m=0.35):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(num_classes, in_dim))\n",
    "        nn.init.normal_(self.weight, std=0.01)\n",
    "        self.s = float(s)\n",
    "        self.m = float(m)\n",
    "\n",
    "    def forward(self, x_normed, labels=None):\n",
    "        # x_normed: (B, D), already normalized\n",
    "        w = F.normalize(self.weight, dim=1)     # (C, D)\n",
    "        cosine = x_normed @ w.t()                 # (B, C)\n",
    "\n",
    "        if labels is None:\n",
    "            # inference\n",
    "            return self.s * cosine\n",
    "\n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1), 1.0)\n",
    "\n",
    "        cosine_m = cosine - one_hot * self.m\n",
    "        return self.s * cosine_m\n",
    "\n",
    "class ReIDNetCosFace(nn.Module):\n",
    "    def __init__(self, emb_dim, num_classes, s=30.0, m=0.35):\n",
    "        super().__init__()\n",
    "        backbone = models.resnet50(\n",
    "            weights=models.ResNet50_Weights.IMAGENET1K_V2\n",
    "        )\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(2048, emb_dim)\n",
    "        self.cls = CosFaceClassifier(emb_dim, num_classes, s=s, m=m)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        f = self.backbone(x)\n",
    "        f = self.pool(f).flatten(1)\n",
    "        z = self.fc(f)\n",
    "        z = F.normalize(z, dim=1)\n",
    "\n",
    "        logits = self.cls(z, labels=labels)\n",
    "        return logits, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### no scheduler (diagnostic run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:54:16.141971Z",
     "iopub.status.busy": "2026-01-29T23:54:16.141506Z",
     "iopub.status.idle": "2026-01-30T00:36:52.777822Z",
     "shell.execute_reply": "2026-01-30T00:36:52.776967Z",
     "shell.execute_reply.started": "2026-01-29T23:54:16.141940Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/158538094.py:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>step</td><td></td></tr><tr><td>train/acc1</td><td></td></tr><tr><td>train/emb_norm_mean</td><td></td></tr><tr><td>train/epoch_acc1_avg</td><td></td></tr><tr><td>train/epoch_loss_avg</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/lr</td><td></td></tr><tr><td>train/max_prob_mean</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>step</td><td>4659</td></tr><tr><td>train/acc1</td><td>0.98305</td></tr><tr><td>train/emb_norm_mean</td><td>1</td></tr><tr><td>train/epoch_acc1_avg</td><td>0.99329</td></tr><tr><td>train/epoch_loss_avg</td><td>0.03438</td></tr><tr><td>train/loss</td><td>0.04846</td></tr><tr><td>train/lr</td><td>3e-05</td></tr><tr><td>train/max_prob_mean</td><td>0.99609</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">reid_r50_cosface_20e_adamw_warmup_step_1769724884</strong> at: <a href='https://wandb.ai/unibo-ai/person%20re-id/runs/7b21mubl' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id/runs/7b21mubl</a><br> View project at: <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260129_221444-7b21mubl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20260129_235416-jay640jh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unibo-ai/person%20re-id/runs/jay640jh' target=\"_blank\">reid_r50_cosface_20e_adamw_constlr_1769730856</a></strong> to <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unibo-ai/person%20re-id/runs/jay640jh' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id/runs/jay640jh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 00] loss=10.5928 acc1=0.1339\n",
      "[Epoch 01] loss=3.8810 acc1=0.4793\n",
      "[Epoch 02] loss=1.9305 acc1=0.6962\n",
      "[Epoch 03] loss=1.1689 acc1=0.7944\n",
      "[Epoch 04] loss=0.7949 acc1=0.8533\n",
      "[CKPT] Saved reid_cosface_no_epoch05.pth\n",
      "[Epoch 05] loss=0.6024 acc1=0.8830\n",
      "[Epoch 06] loss=0.4330 acc1=0.9144\n",
      "[Epoch 07] loss=0.4047 acc1=0.9183\n",
      "[Epoch 08] loss=0.3198 acc1=0.9388\n",
      "[Epoch 09] loss=0.3338 acc1=0.9305\n",
      "[CKPT] Saved reid_cosface_no_epoch10.pth\n",
      "[Epoch 10] loss=0.3339 acc1=0.9342\n",
      "[Epoch 11] loss=0.2548 acc1=0.9467\n",
      "[Epoch 12] loss=0.2368 acc1=0.9524\n",
      "[Epoch 13] loss=0.2390 acc1=0.9555\n",
      "[Epoch 14] loss=0.2338 acc1=0.9528\n",
      "[CKPT] Saved reid_cosface_no_epoch15.pth\n",
      "[Epoch 15] loss=0.2484 acc1=0.9513\n",
      "[Epoch 16] loss=0.2748 acc1=0.9475\n",
      "[Epoch 17] loss=0.2451 acc1=0.9529\n",
      "[Epoch 18] loss=0.2167 acc1=0.9581\n",
      "[Epoch 19] loss=0.1983 acc1=0.9604\n",
      "[CKPT] Saved reid_cosface_no_epoch20.pth\n",
      "[FINAL] Saved reid_20e_cosface_final_no.pth\n"
     ]
    }
   ],
   "source": [
    "# -------------------- LOSS --------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# -------------------- MODEL --------------------\n",
    "model_reid_cf = ReIDNetCosFace(emb_dim=emb_dim, num_classes=num_classes).to(device)\n",
    "\n",
    "# -------------------- OPTIMIZER --------------------\n",
    "optimizer = torch.optim.AdamW(model_reid_cf.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# -------------------- WANDB INIT --------------------\n",
    "run = wandb.init(\n",
    "    entity=\"unibo-ai\",\n",
    "    project=\"person re-id\",\n",
    "    config={\n",
    "        \"seed\": seed,\n",
    "        \"dataset\": \"PRW\",\n",
    "        \"architecture\": \"ReIDNetCosFace\",\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"epochs\": n_epochs,\n",
    "        \"batch_size\": getattr(train_reid_loader, \"batch_size\", None),\n",
    "        \"sampler\": type(getattr(train_reid_loader, \"batch_sampler\", None)).__name__\n",
    "        if getattr(train_reid_loader, \"batch_sampler\", None) is not None else \"shuffle\",\n",
    "        \"resize\": \"(256,128)\",\n",
    "        \"loss\": \"CosFace + CE\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"learning_rate\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"scheduler\": \"NONE (constant LR)\",\n",
    "        \"amp\": use_amp,\n",
    "        \"save_every_epochs\": 5,\n",
    "    },\n",
    "    name=f\"reid_r50_cosface_20e_adamw_constlr_{int(time.time())}\",\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "# -------------------- TRAIN LOOP --------------------\n",
    "global_step = 0\n",
    "save_every = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model_reid_cf.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc1 = 0.0\n",
    "\n",
    "    for crops, labels, pid, camid in train_reid_loader:\n",
    "        crops = crops.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True).view(-1)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "            logits, emb = model_reid_cf(crops, labels=labels)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss_val = float(loss.item())\n",
    "            pred = logits.argmax(dim=1)\n",
    "            acc1 = (pred == labels).float().mean().item()\n",
    "            emb_norm = emb.norm(dim=1).mean().item()\n",
    "            max_prob = F.softmax(logits, dim=1).max(dim=1).values.mean().item()\n",
    "            lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        running_loss += loss_val\n",
    "        running_acc1 += acc1\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train/loss\": loss_val,\n",
    "                \"train/acc1\": acc1,\n",
    "                \"train/emb_norm_mean\": emb_norm,\n",
    "                \"train/max_prob_mean\": max_prob,\n",
    "                \"train/lr\": lr_now,\n",
    "                \"epoch\": epoch,\n",
    "                \"step\": global_step,\n",
    "            },\n",
    "            step=global_step\n",
    "        )\n",
    "        global_step += 1\n",
    "\n",
    "    epoch_loss = running_loss / max(1, len(train_reid_loader))\n",
    "    epoch_acc1 = running_acc1 / max(1, len(train_reid_loader))\n",
    "\n",
    "    wandb.log(\n",
    "        {\"train/epoch_loss_avg\": epoch_loss, \"train/epoch_acc1_avg\": epoch_acc1, \"epoch\": epoch},\n",
    "        step=global_step\n",
    "    )\n",
    "    print(f\"[Epoch {epoch:02d}] loss={epoch_loss:.4f} acc1={epoch_acc1:.4f}\")\n",
    "\n",
    "    # -------------------- SAVE CHECKPOINT EVERY N EPOCHS --------------------\n",
    "    if ((epoch + 1) % save_every) == 0:\n",
    "        ckpt_path = f\"reid_cosface_no_epoch{epoch+1:02d}.pth\"\n",
    "        ckpt = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"global_step\": global_step,\n",
    "            \"model\": model_reid_cf.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scaler\": scaler.state_dict() if use_amp else None,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"emb_dim\": emb_dim,\n",
    "            \"pid2label\": train_reid_ds.pid2label,\n",
    "            \"config\": {\n",
    "                \"seed\": seed,\n",
    "                \"lr\": lr,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"n_epochs\": n_epochs,\n",
    "                \"use_amp\": use_amp,\n",
    "                \"save_every\": save_every,\n",
    "                \"scheduler\": \"NONE\",\n",
    "            },\n",
    "        }\n",
    "        torch.save(ckpt, ckpt_path)\n",
    "\n",
    "        artifact = wandb.Artifact(f\"reid_cosface_epoch{epoch+1:02d}\", type=\"model\")\n",
    "        artifact.add_file(ckpt_path)\n",
    "        wandb.log_artifact(artifact)\n",
    "        print(f\"[CKPT] Saved {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with lr scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T09:35:55.191733Z",
     "iopub.status.busy": "2026-01-30T09:35:55.191418Z",
     "iopub.status.idle": "2026-01-30T10:16:03.083860Z",
     "shell.execute_reply": "2026-01-30T10:16:03.082911Z",
     "shell.execute_reply.started": "2026-01-30T09:35:55.191706Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/983525373.py:42: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">reid_r50_cosface_20e_adamw_warmup_step_ls0.1_wd0.0005_1769765656</strong> at: <a href='https://wandb.ai/unibo-ai/person%20re-id/runs/mlqvnuas' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id/runs/mlqvnuas</a><br> View project at: <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260130_093416-mlqvnuas/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20260130_093555-7qvopfe2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unibo-ai/person%20re-id/runs/7qvopfe2' target=\"_blank\">reid_r50_cosface_20e_adamw_warmup_step_ls0.1_wd0.0005_1769765755</a></strong> to <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unibo-ai/person%20re-id/runs/7qvopfe2' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id/runs/7qvopfe2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 00] loss=13.3464 acc1=0.0435\n",
      "[Epoch 01] loss=6.2479 acc1=0.3279\n",
      "[Epoch 02] loss=3.4681 acc1=0.6118\n",
      "[Epoch 03] loss=2.5046 acc1=0.7420\n",
      "[Epoch 04] loss=2.0118 acc1=0.8200\n",
      "[CKPT] Saved reid_cosface_lswd_epoch05.pth\n",
      "[Epoch 05] loss=1.7208 acc1=0.8706\n",
      "[Epoch 06] loss=1.5868 acc1=0.9000\n",
      "[Epoch 07] loss=1.5217 acc1=0.9099\n",
      "[Epoch 08] loss=1.5353 acc1=0.9068\n",
      "[Epoch 09] loss=1.4520 acc1=0.9241\n",
      "[CKPT] Saved reid_cosface_lswd_epoch10.pth\n",
      "[Epoch 10] loss=1.3593 acc1=0.9421\n",
      "[Epoch 11] loss=1.3709 acc1=0.9403\n",
      "[Epoch 12] loss=1.4277 acc1=0.9291\n",
      "[Epoch 13] loss=1.3936 acc1=0.9333\n",
      "[Epoch 14] loss=1.3486 acc1=0.9456\n",
      "[CKPT] Saved reid_cosface_lswd_epoch15.pth\n",
      "[Epoch 15] loss=1.3377 acc1=0.9473\n",
      "[Epoch 16] loss=1.2968 acc1=0.9548\n",
      "[Epoch 17] loss=1.3559 acc1=0.9435\n",
      "[Epoch 18] loss=1.3093 acc1=0.9497\n",
      "[Epoch 19] loss=1.2974 acc1=0.9521\n",
      "[CKPT] Saved reid_cosface_lswd_epoch20.pth\n"
     ]
    }
   ],
   "source": [
    "# -------------------- LOSS (label smoothing) --------------------\n",
    "label_smoothing = 0.1\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "# -------------------- MODEL --------------------\n",
    "model_reid_cf = ReIDNetCosFace(emb_dim=emb_dim, num_classes=num_classes).to(device)\n",
    "\n",
    "# -------------------- OPTIMIZER (higher weight decay) --------------------\n",
    "optimizer = torch.optim.AdamW(model_reid_cf.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# -------------------- LR SCHEDULER: warmup (per-iter) + step drop (per-epoch) --------------------\n",
    "steps_per_epoch = len(train_reid_loader)\n",
    "\n",
    "warmup_epochs = 1\n",
    "warmup_steps = warmup_epochs * steps_per_epoch\n",
    "\n",
    "def warmup_lambda(step: int) -> float:\n",
    "    if step < warmup_steps:\n",
    "        return (step + 1) / max(1, warmup_steps)\n",
    "    return 1.0\n",
    "\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
    "\n",
    "milestones = [16]\n",
    "gamma = 0.1\n",
    "step_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "\n",
    "# -------------------- WANDB INIT --------------------\n",
    "run = wandb.init(\n",
    "    entity=\"unibo-ai\",\n",
    "    project=\"person re-id\",\n",
    "    config={\n",
    "        \"seed\": seed,\n",
    "        \"dataset\": \"PRW\",\n",
    "        \"architecture\": \"ReIDNetCosFace\",\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"epochs\": n_epochs,\n",
    "        \"batch_size\": getattr(train_reid_loader, \"batch_size\", None),\n",
    "        \"sampler\": type(getattr(train_reid_loader, \"batch_sampler\", None)).__name__\n",
    "        if getattr(train_reid_loader, \"batch_sampler\", None) is not None else \"shuffle\",\n",
    "        \"resize\": \"(256,128)\",\n",
    "        \"loss\": f\"CosFace + CE (label_smoothing={label_smoothing})\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"learning_rate\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"scheduler\": \"warmup(1 epoch, per-iter) + MultiStepLR(milestone=16, gamma=0.1)\",\n",
    "        \"warmup_epochs\": warmup_epochs,\n",
    "        \"milestones\": milestones,\n",
    "        \"gamma\": gamma,\n",
    "        \"amp\": use_amp,\n",
    "        \"save_every_epochs\": 5,\n",
    "    },\n",
    "    name=f\"reid_r50_cosface_20e_adamw_warmup_step_ls{label_smoothing}_wd{weight_decay}_{int(time.time())}\",\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "# -------------------- TRAIN LOOP --------------------\n",
    "global_step = 0\n",
    "save_every = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model_reid_cf.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc1 = 0.0\n",
    "\n",
    "    for crops, labels, pid, camid in train_reid_loader:\n",
    "        crops = crops.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True).view(-1)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "            logits, emb = model_reid_cf(crops, labels=labels)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        # --- optimizer step (AMP-safe) ---\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)      # <-- optimizer.step()\n",
    "        scaler.update()\n",
    "\n",
    "        # --- scheduler step: AFTER optimizer.step() ---\n",
    "        warmup_scheduler.step()     # <-- always ok (lambda returns 1.0 after warmup)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss_val = float(loss.item())\n",
    "            pred = logits.argmax(dim=1)\n",
    "            acc1 = (pred == labels).float().mean().item()\n",
    "            emb_norm = emb.norm(dim=1).mean().item()\n",
    "            max_prob = F.softmax(logits, dim=1).max(dim=1).values.mean().item()\n",
    "            lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        running_loss += loss_val\n",
    "        running_acc1 += acc1\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train/loss\": loss_val,\n",
    "                \"train/acc1\": acc1,\n",
    "                \"train/emb_norm_mean\": emb_norm,\n",
    "                \"train/max_prob_mean\": max_prob,\n",
    "                \"train/lr\": lr_now,\n",
    "                \"epoch\": epoch,\n",
    "                \"step\": global_step,\n",
    "            },\n",
    "            step=global_step\n",
    "        )\n",
    "        global_step += 1\n",
    "\n",
    "    # --- end epoch ---\n",
    "    epoch_loss = running_loss / max(1, len(train_reid_loader))\n",
    "    epoch_acc1 = running_acc1 / max(1, len(train_reid_loader))\n",
    "\n",
    "    wandb.log(\n",
    "        {\"train/epoch_loss_avg\": epoch_loss, \"train/epoch_acc1_avg\": epoch_acc1, \"epoch\": epoch},\n",
    "        step=global_step\n",
    "    )\n",
    "    print(f\"[Epoch {epoch:02d}] loss={epoch_loss:.4f} acc1={epoch_acc1:.4f}\")\n",
    "\n",
    "    # step scheduler per-epoch after warmup\n",
    "    if (epoch + 1) > warmup_epochs:\n",
    "        step_scheduler.step()\n",
    "\n",
    "    # --- checkpoint ---\n",
    "    if ((epoch + 1) % save_every) == 0:\n",
    "        ckpt_path = f\"reid_cosface_lswd_epoch{epoch+1:02d}.pth\"\n",
    "        ckpt = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"global_step\": global_step,\n",
    "            \"model\": model_reid_cf.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"warmup_scheduler\": warmup_scheduler.state_dict(),\n",
    "            \"step_scheduler\": step_scheduler.state_dict(),\n",
    "            \"scaler\": scaler.state_dict() if use_amp else None,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"emb_dim\": emb_dim,\n",
    "            \"pid2label\": train_reid_ds.pid2label,\n",
    "        }\n",
    "        torch.save(ckpt, ckpt_path)\n",
    "\n",
    "        artifact = wandb.Artifact(f\"reid_cosface_epoch{epoch+1:02d}\", type=\"model\")\n",
    "        artifact.add_file(ckpt_path)\n",
    "        wandb.log_artifact(artifact)\n",
    "        print(f\"[CKPT] Saved {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-30T10:18:55.851503Z",
     "iopub.status.busy": "2026-01-30T10:18:55.851088Z",
     "iopub.status.idle": "2026-01-30T10:19:28.560285Z",
     "shell.execute_reply": "2026-01-30T10:19:28.559170Z",
     "shell.execute_reply.started": "2026-01-30T10:18:55.851459Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/2725153609.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 4) Compute gallery features aligned with detector outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m test_gallery_feats = build_gallery_feats_from_dets_prw_dataset(\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mreid_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_reid_cf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprw_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_55/1235641935.py\u001b[0m in \u001b[0;36mbuild_gallery_feats_from_dets_prw_dataset\u001b[0;34m(reid_model, prw_dataset, detections, transform, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreid_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# <-- 512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# useful for sanity check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mfeats_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_55/1825657938.py\u001b[0m in \u001b[0;36mforward_embed\u001b[0;34m(model, batch)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'Call this to ignore logits but consider the embedding.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_55/1972847860.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, labels)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2824\u001b[0m         \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2825\u001b[0m         \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2826\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2827\u001b[0m     )\n\u001b[1;32m   2828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compute query features from query_box (NO crop-from-frame)\n",
    "query_feats = compute_query_box_feats_from_querybox(\n",
    "    reid_model=model_reid_cf,\n",
    "    query_info_path=\"/kaggle/input/prw-person-re-identification-in-the-wild/query_info.txt\",\n",
    "    query_box_dir=\"/kaggle/input/prw-person-re-identification-in-the-wild/query_box\",\n",
    "    transform=test_reid_tf,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Compute gallery features aligned with detector outputs\n",
    "test_gallery_feats = build_gallery_feats_from_dets_prw_dataset(\n",
    "    reid_model=model_reid_cf,\n",
    "    prw_dataset=test_ds,\n",
    "    detections=test_detections,\n",
    "    transform=test_reid_tf,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"query dim:\", query_feats[0].shape)              # (512,)\n",
    "print(\"gallery dim:\", test_gallery_feats[0].shape)          # (N,512)\n",
    "print({f.shape[1] for f in test_gallery_feats if f.shape[0] > 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T21:45:34.015480Z",
     "iopub.status.busy": "2026-01-28T21:45:34.015127Z",
     "iopub.status.idle": "2026-01-28T21:48:39.016148Z",
     "shell.execute_reply": "2026-01-28T21:48:39.014724Z",
     "shell.execute_reply.started": "2026-01-28T21:45:34.015446Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search ranking:\n",
      "  mAP = 42.54%\n",
      "  top- 1 = 83.08%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation: 5 epochs\n",
    "ret_cf = eval_search_prw(\n",
    "    gallery_dataset=gallery_eval,\n",
    "    query_dataset=query_ds,\n",
    "    gallery_dets=test_detections,\n",
    "    gallery_feats=test_gallery_feats,\n",
    "    query_box_feats=query_feats,\n",
    "    det_thresh=0.3,\n",
    "    ignore_cam_id=True, # simplyfing the problem: using same camera\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **[5 epochs / no lr scheduler setting]** Although CosFace slows down convergence by introducing an explicit angular margin during training, it improves mAP ( +1.1%) because it enforces tighter intra-class compactness and larger inter-class separation in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T10:20:01.301507Z",
     "iopub.status.busy": "2026-01-30T10:20:01.301213Z",
     "iopub.status.idle": "2026-01-30T10:52:59.625141Z",
     "shell.execute_reply": "2026-01-30T10:52:59.624449Z",
     "shell.execute_reply.started": "2026-01-30T10:20:01.301480Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Eval] Loading checkpoint: reid_cosface_lswd_epoch20.pth\n",
      "search ranking:\n",
      "  mAP = 31.56%\n",
      "  top- 1 = 75.98%\n",
      "[Eval] reid_cosface_lswd_epoch20.pth  mAP=31.56% | top-1=75.98%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_cosface_lswd_epoch05.pth\n",
      "search ranking:\n",
      "  mAP = 41.76%\n",
      "  top- 1 = 83.28%\n",
      "[Eval] reid_cosface_lswd_epoch05.pth  mAP=41.76% | top-1=83.28%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_cosface_lswd_epoch10.pth\n",
      "search ranking:\n",
      "  mAP = 37.26%\n",
      "  top- 1 = 79.92%\n",
      "[Eval] reid_cosface_lswd_epoch10.pth  mAP=37.26% | top-1=79.92%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_cosface_lswd_epoch15.pth\n",
      "search ranking:\n",
      "  mAP = 35.07%\n",
      "  top- 1 = 78.66%\n",
      "[Eval] reid_cosface_lswd_epoch15.pth  mAP=35.07% | top-1=78.66%\n"
     ]
    }
   ],
   "source": [
    "ckpt_epochs = [20, 5, 10, 15]\n",
    "ckpt_paths  = [f\"reid_cosface_lswd_epoch{e:02d}.pth\" for e in ckpt_epochs]\n",
    "\n",
    "# Evaluate model\n",
    "results = {}\n",
    "for epoch, ckpt_path in zip(ckpt_epochs, ckpt_paths):\n",
    "    ret = evaluate_checkpoint_prw(\n",
    "        ckpt_path=ckpt_path,\n",
    "        model=model_reid_cf,\n",
    "        query_ds=query_ds,\n",
    "        gallery_eval=gallery_eval,\n",
    "        test_ds=test_ds,\n",
    "        test_detections=test_detections,\n",
    "        test_reid_tf=test_reid_tf,\n",
    "        device=device,\n",
    "    )\n",
    "    results[epoch] = ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArcFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:22:03.555928Z",
     "iopub.status.busy": "2026-02-10T10:22:03.555176Z",
     "iopub.status.idle": "2026-02-10T10:22:03.561185Z",
     "shell.execute_reply": "2026-02-10T10:22:03.560607Z",
     "shell.execute_reply.started": "2026-02-10T10:22:03.555895Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ReIDNetArcFace(nn.Module):\n",
    "    def __init__(self, emb_dim, num_classes, s=30.0, m=0.35):\n",
    "        super().__init__()\n",
    "        backbone = models.resnet50(\n",
    "            weights=models.ResNet50_Weights.IMAGENET1K_V2\n",
    "        )\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(2048, emb_dim)\n",
    "\n",
    "        self.cls = ArcFaceClassifier(\n",
    "            in_dim=emb_dim,\n",
    "            num_classes=num_classes,\n",
    "            s=s,\n",
    "            m=m\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        f = self.backbone(x)\n",
    "        f = self.pool(f).flatten(1)\n",
    "        z = self.fc(f)\n",
    "        z = F.normalize(z, dim=1)  # IMPORTANT\n",
    "\n",
    "        logits = self.cls(z, labels=labels)\n",
    "        return logits, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:22:07.528530Z",
     "iopub.status.busy": "2026-02-10T10:22:07.527953Z",
     "iopub.status.idle": "2026-02-10T10:22:07.535070Z",
     "shell.execute_reply": "2026-02-10T10:22:07.534484Z",
     "shell.execute_reply.started": "2026-02-10T10:22:07.528498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ArcFaceClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, num_classes, s=30.0, m=0.35):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(num_classes, in_dim))\n",
    "        nn.init.normal_(self.weight, std=0.01)\n",
    "\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "\n",
    "        # cos(m) and sin(m) are constants\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, x_normed, labels=None):\n",
    "        # x_normed: (B, D), already L2-normalized\n",
    "        w = F.normalize(self.weight, dim=1)        # (C, D)\n",
    "        cosine = torch.matmul(x_normed, w.t())     # (B, C)\n",
    "\n",
    "        if labels is None:\n",
    "            # inference\n",
    "            return self.s * cosine\n",
    "\n",
    "        # ---- ArcFace margin ----\n",
    "        sine = torch.sqrt(1.0 - torch.clamp(cosine ** 2, 0, 1))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m  # cos(theta + m)\n",
    "\n",
    "        # optional safeguard (standard ArcFace trick)\n",
    "        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "\n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1), 1.0)\n",
    "\n",
    "        logits = one_hot * phi + (1.0 - one_hot) * cosine\n",
    "        return self.s * logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T11:03:47.207692Z",
     "iopub.status.busy": "2026-01-30T11:03:47.207385Z",
     "iopub.status.idle": "2026-01-30T11:47:31.500756Z",
     "shell.execute_reply": "2026-01-30T11:47:31.499910Z",
     "shell.execute_reply.started": "2026-01-30T11:03:47.207651Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 97.8M/97.8M [00:00<00:00, 182MB/s] \n",
      "/tmp/ipykernel_55/33651200.py:45: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n",
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtommaso-perniola\u001b[0m (\u001b[33munibo-ai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20260130_110356-1rzph9c6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unibo-ai/person%20re-id/runs/1rzph9c6' target=\"_blank\">reid_r50_arcface_20e_adamw_warmup_step_1769771028</a></strong> to <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unibo-ai/person%20re-id/runs/1rzph9c6' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id/runs/1rzph9c6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 00] loss=13.7115 acc1=0.0506\n",
      "[Epoch 01] loss=5.1394 acc1=0.3917\n",
      "[Epoch 02] loss=2.0924 acc1=0.6747\n",
      "[Epoch 03] loss=1.1289 acc1=0.8041\n",
      "[Epoch 04] loss=0.7265 acc1=0.8612\n",
      "[CKPT] Saved reid_arcface_epoch05.pth\n",
      "[Epoch 05] loss=0.5186 acc1=0.8973\n",
      "[Epoch 06] loss=0.3595 acc1=0.9264\n",
      "[Epoch 07] loss=0.3500 acc1=0.9288\n",
      "[Epoch 08] loss=0.3073 acc1=0.9353\n",
      "[Epoch 09] loss=0.2851 acc1=0.9392\n",
      "[CKPT] Saved reid_arcface_epoch10.pth\n",
      "[Epoch 10] loss=0.2803 acc1=0.9409\n",
      "[Epoch 11] loss=0.2621 acc1=0.9451\n",
      "[Epoch 12] loss=0.2280 acc1=0.9536\n",
      "[Epoch 13] loss=0.2283 acc1=0.9514\n",
      "[Epoch 14] loss=0.2088 acc1=0.9589\n",
      "[CKPT] Saved reid_arcface_epoch15.pth\n",
      "[Epoch 15] loss=0.1938 acc1=0.9586\n",
      "[Epoch 16] loss=0.1713 acc1=0.9634\n",
      "[Epoch 17] loss=0.0703 acc1=0.9856\n",
      "[Epoch 18] loss=0.0285 acc1=0.9943\n",
      "[Epoch 19] loss=0.0196 acc1=0.9964\n",
      "[CKPT] Saved reid_arcface_epoch20.pth\n",
      "[FINAL] Saved reid_arcface_final.pth\n"
     ]
    }
   ],
   "source": [
    "# -------------------- LOSS --------------------\n",
    "# aligned with CosFace experiments\n",
    "label_smoothing = 0.0  \n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "# -------------------- MODEL --------------------\n",
    "model_reid_af = ReIDNetArcFace(emb_dim=emb_dim, num_classes=num_classes, s=30.0, m=0.35).to(device)\n",
    "\n",
    "# -------------------- OPTIMIZER --------------------\n",
    "optimizer = torch.optim.AdamW(model_reid_af.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# -------------------- LR SCHEDULER: warmup (per-iter) + step drop (per-epoch) --------------------\n",
    "steps_per_epoch = len(train_reid_loader)\n",
    "\n",
    "warmup_epochs = 1\n",
    "warmup_steps = warmup_epochs * steps_per_epoch\n",
    "\n",
    "def warmup_lambda(step: int) -> float:\n",
    "    if step < warmup_steps:\n",
    "        return (step + 1) / max(1, warmup_steps)\n",
    "    return 1.0\n",
    "\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
    "\n",
    "milestones = [16]  # drop at start of epoch 17 (when stepping at end of epoch)\n",
    "gamma = 0.1\n",
    "step_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "\n",
    "# -------------------- WANDB INIT --------------------\n",
    "run = wandb.init(\n",
    "    entity=\"unibo-ai\",\n",
    "    project=\"person re-id\",\n",
    "    config={\n",
    "        \"seed\": seed,\n",
    "        \"dataset\": \"PRW\",\n",
    "        \"architecture\": \"ReIDNetArcFace\",\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"epochs\": n_epochs,\n",
    "        \"batch_size\": getattr(train_reid_loader, \"batch_size\", None),\n",
    "        \"sampler\": type(getattr(train_reid_loader, \"batch_sampler\", None)).__name__\n",
    "        if getattr(train_reid_loader, \"batch_sampler\", None) is not None else \"shuffle\",\n",
    "        \"resize\": \"(256,128)\",\n",
    "        \"loss\": f\"ArcFace + CE (label_smoothing={label_smoothing})\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"learning_rate\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"scheduler\": \"warmup(1 epoch, per-iter) + MultiStepLR(milestone=16, gamma=0.1)\",\n",
    "        \"warmup_epochs\": warmup_epochs,\n",
    "        \"milestones\": milestones,\n",
    "        \"gamma\": gamma,\n",
    "        \"amp\": use_amp,\n",
    "        \"save_every_epochs\": 5,\n",
    "        \"arcface_s\": 30.0,\n",
    "        \"arcface_m\": 0.35,\n",
    "    },\n",
    "    name=f\"reid_r50_arcface_20e_adamw_warmup_step_{int(time.time())}\",\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "# -------------------- TRAIN LOOP --------------------\n",
    "global_step = 0\n",
    "save_every = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model_reid_af.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc1 = 0.0\n",
    "\n",
    "    for crops, labels, pid, camid in train_reid_loader:\n",
    "        crops = crops.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True).view(-1)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "            # ArcFace needs labels to apply the angular margin (like CosFace)\n",
    "            logits, emb = model_reid_af(crops, labels=labels)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # per-iter warmup only\n",
    "        if global_step < warmup_steps:\n",
    "            warmup_scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss_val = float(loss.item())\n",
    "            pred = logits.argmax(dim=1)\n",
    "            acc1 = (pred == labels).float().mean().item()\n",
    "            emb_norm = emb.norm(dim=1).mean().item()\n",
    "            max_prob = F.softmax(logits, dim=1).max(dim=1).values.mean().item()\n",
    "            lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        running_loss += loss_val\n",
    "        running_acc1 += acc1\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train/loss\": loss_val,\n",
    "                \"train/acc1\": acc1,\n",
    "                \"train/emb_norm_mean\": emb_norm,\n",
    "                \"train/max_prob_mean\": max_prob,\n",
    "                \"train/lr\": lr_now,\n",
    "                \"epoch\": epoch,\n",
    "                \"step\": global_step,\n",
    "            },\n",
    "            step=global_step\n",
    "        )\n",
    "        global_step += 1\n",
    "\n",
    "    epoch_loss = running_loss / max(1, len(train_reid_loader))\n",
    "    epoch_acc1 = running_acc1 / max(1, len(train_reid_loader))\n",
    "\n",
    "    wandb.log(\n",
    "        {\"train/epoch_loss_avg\": epoch_loss, \"train/epoch_acc1_avg\": epoch_acc1, \"epoch\": epoch},\n",
    "        step=global_step\n",
    "    )\n",
    "    print(f\"[Epoch {epoch:02d}] loss={epoch_loss:.4f} acc1={epoch_acc1:.4f}\")\n",
    "\n",
    "    # per-epoch step decay after warmup epoch(s)\n",
    "    if (epoch + 1) > warmup_epochs:\n",
    "        step_scheduler.step()\n",
    "\n",
    "    # -------------------- SAVE CHECKPOINTS --------------------\n",
    "    if ((epoch + 1) % save_every) == 0:\n",
    "        ckpt_path = f\"reid_arcface_epoch{epoch+1:02d}.pth\"\n",
    "        ckpt = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"global_step\": global_step,\n",
    "            \"model\": model_reid_af.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"warmup_scheduler\": warmup_scheduler.state_dict(),\n",
    "            \"step_scheduler\": step_scheduler.state_dict(),\n",
    "            \"scaler\": scaler.state_dict() if use_amp else None,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"emb_dim\": emb_dim,\n",
    "            \"pid2label\": train_reid_ds.pid2label,\n",
    "            \"config\": {\n",
    "                \"seed\": seed,\n",
    "                \"lr\": lr,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"label_smoothing\": label_smoothing,\n",
    "                \"n_epochs\": n_epochs,\n",
    "                \"use_amp\": use_amp,\n",
    "                \"save_every\": save_every,\n",
    "                \"warmup_epochs\": warmup_epochs,\n",
    "                \"milestones\": milestones,\n",
    "                \"gamma\": gamma,\n",
    "                \"arcface_s\": 30.0,\n",
    "                \"arcface_m\": 0.35,\n",
    "            },\n",
    "        }\n",
    "        torch.save(ckpt, ckpt_path)\n",
    "        artifact = wandb.Artifact(f\"reid_arcface_epoch{epoch+1:02d}\", type=\"model\")\n",
    "        artifact.add_file(ckpt_path)\n",
    "        wandb.log_artifact(artifact)\n",
    "        print(f\"[CKPT] Saved {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T11:56:43.547295Z",
     "iopub.status.busy": "2026-01-30T11:56:43.546563Z",
     "iopub.status.idle": "2026-01-30T12:33:43.956161Z",
     "shell.execute_reply": "2026-01-30T12:33:43.955399Z",
     "shell.execute_reply.started": "2026-01-30T11:56:43.547267Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Eval] Loading checkpoint: reid_arcface_epoch05.pth\n",
      "search ranking:\n",
      "  mAP = 43.03%\n",
      "  top- 1 = 82.79%\n",
      "[Eval] reid_arcface_epoch05.pth  mAP=43.03% | top-1=82.79%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_arcface_epoch10.pth\n",
      "search ranking:\n",
      "  mAP = 40.98%\n",
      "  top- 1 = 80.51%\n",
      "[Eval] reid_arcface_epoch10.pth  mAP=40.98% | top-1=80.51%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_arcface_epoch15.pth\n",
      "search ranking:\n",
      "  mAP = 41.21%\n",
      "  top- 1 = 81.09%\n",
      "[Eval] reid_arcface_epoch15.pth  mAP=41.21% | top-1=81.09%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_arcface_epoch20.pth\n",
      "search ranking:\n",
      "  mAP = 41.73%\n",
      "  top- 1 = 81.09%\n",
      "[Eval] reid_arcface_epoch20.pth  mAP=41.73% | top-1=81.09%\n"
     ]
    }
   ],
   "source": [
    "ckpt_epochs = [5, 10, 15, 20]\n",
    "ckpt_paths  = [f\"reid_arcface_epoch{e:02d}.pth\" for e in ckpt_epochs]\n",
    "\n",
    "results = {}\n",
    "for epoch, ckpt_path in zip(ckpt_epochs, ckpt_paths):\n",
    "    ret = evaluate_checkpoint_prw(\n",
    "        ckpt_path=ckpt_path,\n",
    "        model=model_reid_af,\n",
    "        query_ds=query_ds,\n",
    "        gallery_eval=gallery_eval,\n",
    "        test_ds=test_ds,\n",
    "        test_detections=test_detections,\n",
    "        test_reid_tf=test_reid_tf,\n",
    "        device=device,\n",
    "    )\n",
    "    results[epoch] = ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing margin to m = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T13:57:58.553865Z",
     "iopub.status.busy": "2026-01-30T13:57:58.553561Z",
     "iopub.status.idle": "2026-01-30T14:39:52.453273Z",
     "shell.execute_reply": "2026-01-30T14:39:52.452551Z",
     "shell.execute_reply.started": "2026-01-30T13:57:58.553839Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 97.8M/97.8M [00:00<00:00, 177MB/s] \n",
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtommaso-perniola\u001b[0m (\u001b[33munibo-ai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20260130_135808-q2dsuk1n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unibo-ai/person%20re-id/runs/q2dsuk1n' target=\"_blank\">reid_r50_arcface_20e_m25_adamw_warmup_step_1769781480</a></strong> to <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unibo-ai/person%20re-id/runs/q2dsuk1n' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id/runs/q2dsuk1n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 00] loss=10.9921 acc1=0.0643\n",
      "[Epoch 01] loss=3.4852 acc1=0.4768\n",
      "[Epoch 02] loss=1.2967 acc1=0.7522\n",
      "[Epoch 03] loss=0.6656 acc1=0.8563\n",
      "[Epoch 04] loss=0.3760 acc1=0.9137\n",
      "[CKPT] Saved reid_arcface_m25_epoch05.pth\n",
      "[Epoch 05] loss=0.2685 acc1=0.9361\n",
      "[Epoch 06] loss=0.2290 acc1=0.9467\n",
      "[Epoch 07] loss=0.2146 acc1=0.9502\n",
      "[Epoch 08] loss=0.1706 acc1=0.9601\n",
      "[Epoch 09] loss=0.1245 acc1=0.9705\n",
      "[CKPT] Saved reid_arcface_m25_epoch10.pth\n",
      "[Epoch 10] loss=0.1755 acc1=0.9587\n",
      "[Epoch 11] loss=0.1622 acc1=0.9594\n",
      "[Epoch 12] loss=0.1828 acc1=0.9575\n",
      "[Epoch 13] loss=0.1533 acc1=0.9644\n",
      "[Epoch 14] loss=0.1322 acc1=0.9677\n",
      "[CKPT] Saved reid_arcface_m25_epoch15.pth\n",
      "[Epoch 15] loss=0.1662 acc1=0.9622\n",
      "[Epoch 16] loss=0.1305 acc1=0.9677\n",
      "[Epoch 17] loss=0.0501 acc1=0.9862\n",
      "[Epoch 18] loss=0.0187 acc1=0.9958\n",
      "[Epoch 19] loss=0.0158 acc1=0.9962\n",
      "[CKPT] Saved reid_arcface_m25_epoch20.pth\n",
      "[FINAL] Saved reid_arcface_m25_final.pth\n"
     ]
    }
   ],
   "source": [
    "# -------------------- LOSS --------------------\n",
    "label_smoothing = 0.0\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "# -------------------- MODEL --------------------\n",
    "num_classes = len(train_reid_ds.pids)\n",
    "emb_dim = 512\n",
    "arc_s = 30.0\n",
    "arc_m = 0.25\n",
    "\n",
    "model_reid_af = ReIDNetArcFace(\n",
    "    emb_dim=emb_dim,\n",
    "    num_classes=num_classes,\n",
    "    s=arc_s,\n",
    "    m=arc_m\n",
    ").to(device)\n",
    "\n",
    "# -------------------- OPTIMIZER --------------------\n",
    "optimizer = torch.optim.AdamW(model_reid_af.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# -------------------- LR SCHEDULER: warmup (per-iter) + step drop (per-epoch) --------------------\n",
    "steps_per_epoch = len(train_reid_loader)\n",
    "\n",
    "warmup_epochs = 1\n",
    "warmup_steps = warmup_epochs * steps_per_epoch\n",
    "\n",
    "def warmup_lambda(step: int) -> float:\n",
    "    if step < warmup_steps:\n",
    "        return (step + 1) / max(1, warmup_steps)\n",
    "    return 1.0\n",
    "\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
    "\n",
    "milestones = [16]\n",
    "gamma = 0.1\n",
    "step_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "\n",
    "# -------------------- WANDB INIT --------------------\n",
    "run = wandb.init(\n",
    "    entity=\"unibo-ai\",\n",
    "    project=\"person re-id\",\n",
    "    config={\n",
    "        \"seed\": seed,\n",
    "        \"dataset\": \"PRW\",\n",
    "        \"architecture\": \"ReIDNetArcFace\",\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"epochs\": n_epochs,\n",
    "        \"batch_size\": getattr(train_reid_loader, \"batch_size\", None),\n",
    "        \"sampler\": type(getattr(train_reid_loader, \"batch_sampler\", None)).__name__\n",
    "        if getattr(train_reid_loader, \"batch_sampler\", None) is not None else \"shuffle\",\n",
    "        \"resize\": \"(256,128)\",\n",
    "        \"loss\": f\"ArcFace + CE (label_smoothing={label_smoothing})\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"learning_rate\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"scheduler\": \"warmup(1 epoch, per-iter) + MultiStepLR(milestone=16, gamma=0.1)\",\n",
    "        \"warmup_epochs\": warmup_epochs,\n",
    "        \"milestones\": milestones,\n",
    "        \"gamma\": gamma,\n",
    "        \"amp\": use_amp,\n",
    "        \"save_every_epochs\": 5,\n",
    "        \"arcface_s\": arc_s,\n",
    "        \"arcface_m\": arc_m,\n",
    "    },\n",
    "    name=f\"reid_r50_arcface_20e_m25_adamw_warmup_step_{int(time.time())}\",\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "# -------------------- TRAIN LOOP --------------------\n",
    "global_step = 0\n",
    "save_every = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model_reid_af.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc1 = 0.0\n",
    "\n",
    "    for crops, labels, pid, camid in train_reid_loader:\n",
    "        crops = crops.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True).view(-1)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "            logits, emb = model_reid_af(crops, labels=labels)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        # ---- AMP step ----\n",
    "        prev_scale = scaler.get_scale()\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.step(optimizer)  \n",
    "        scaler.update()\n",
    "\n",
    "        new_scale = scaler.get_scale()\n",
    "        optimizer_was_stepped = (new_scale >= prev_scale)\n",
    "\n",
    "        if optimizer_was_stepped and (global_step < warmup_steps):\n",
    "            warmup_scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss_val = float(loss.item())\n",
    "            pred = logits.argmax(dim=1)\n",
    "            acc1 = (pred == labels).float().mean().item()\n",
    "            emb_norm = emb.norm(dim=1).mean().item()\n",
    "            max_prob = F.softmax(logits, dim=1).max(dim=1).values.mean().item()\n",
    "            lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        running_loss += loss_val\n",
    "        running_acc1 += acc1\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train/loss\": loss_val,\n",
    "                \"train/acc1\": acc1,\n",
    "                \"train/emb_norm_mean\": emb_norm,\n",
    "                \"train/max_prob_mean\": max_prob,\n",
    "                \"train/lr\": lr_now,\n",
    "                \"epoch\": epoch,\n",
    "                \"step\": global_step,\n",
    "            },\n",
    "            step=global_step\n",
    "        )\n",
    "        global_step += 1\n",
    "\n",
    "    epoch_loss = running_loss / max(1, len(train_reid_loader))\n",
    "    epoch_acc1 = running_acc1 / max(1, len(train_reid_loader))\n",
    "\n",
    "    wandb.log(\n",
    "        {\"train/epoch_loss_avg\": epoch_loss, \"train/epoch_acc1_avg\": epoch_acc1, \"epoch\": epoch},\n",
    "        step=global_step\n",
    "    )\n",
    "    print(f\"[Epoch {epoch:02d}] loss={epoch_loss:.4f} acc1={epoch_acc1:.4f}\")\n",
    "\n",
    "    # per-epoch step decay after warmup epoch(s)\n",
    "    if (epoch + 1) > warmup_epochs:\n",
    "        step_scheduler.step()\n",
    "\n",
    "    # -------------------- SAVE CHECKPOINTS --------------------\n",
    "    if ((epoch + 1) % save_every) == 0:\n",
    "        ckpt_path = f\"reid_arcface_m25_epoch{epoch+1:02d}.pth\" \n",
    "        ckpt = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"global_step\": global_step,\n",
    "            \"model\": model_reid_af.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"warmup_scheduler\": warmup_scheduler.state_dict(),\n",
    "            \"step_scheduler\": step_scheduler.state_dict(),\n",
    "            \"scaler\": scaler.state_dict() if use_amp else None,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"emb_dim\": emb_dim,\n",
    "            \"pid2label\": train_reid_ds.pid2label,\n",
    "            \"config\": {\n",
    "                \"seed\": seed,\n",
    "                \"lr\": lr,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"label_smoothing\": label_smoothing,\n",
    "                \"n_epochs\": n_epochs,\n",
    "                \"use_amp\": use_amp,\n",
    "                \"save_every\": save_every,\n",
    "                \"warmup_epochs\": warmup_epochs,\n",
    "                \"milestones\": milestones,\n",
    "                \"gamma\": gamma,\n",
    "                \"arcface_s\": arc_s,\n",
    "                \"arcface_m\": arc_m,\n",
    "            },\n",
    "        }\n",
    "        torch.save(ckpt, ckpt_path)\n",
    "\n",
    "        artifact = wandb.Artifact(f\"reid_arcface_m25_epoch{epoch+1:02d}\", type=\"model\")\n",
    "        artifact.add_file(ckpt_path)\n",
    "        wandb.log_artifact(artifact)\n",
    "        print(f\"[CKPT] Saved {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T14:42:29.412797Z",
     "iopub.status.busy": "2026-01-30T14:42:29.412479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Eval] Loading checkpoint: reid_arcface_m25_epoch05.pth\n",
      "search ranking:\n",
      "  mAP = 43.74%\n",
      "  top- 1 = 83.62%\n",
      "[Eval] reid_arcface_m25_epoch05.pth  mAP=43.74% | top-1=83.62%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_arcface_m25_epoch10.pth\n",
      "search ranking:\n",
      "  mAP = 43.31%\n",
      "  top- 1 = 81.87%\n",
      "[Eval] reid_arcface_m25_epoch10.pth  mAP=43.31% | top-1=81.87%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_arcface_m25_epoch15.pth\n",
      "search ranking:\n",
      "  mAP = 42.48%\n",
      "  top- 1 = 82.64%\n",
      "[Eval] reid_arcface_m25_epoch15.pth  mAP=42.48% | top-1=82.64%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_arcface_m25_epoch20.pth\n"
     ]
    }
   ],
   "source": [
    "ckpt_epochs = [5, 10, 15, 20]\n",
    "ckpt_paths  = [f\"reid_arcface_m25_epoch{e:02d}.pth\" for e in ckpt_epochs]\n",
    "\n",
    "results = {}\n",
    "for epoch, ckpt_path in zip(ckpt_epochs, ckpt_paths):\n",
    "    ret = evaluate_checkpoint_prw(\n",
    "        ckpt_path=ckpt_path,\n",
    "        model=model_reid_af,\n",
    "        query_ds=query_ds,\n",
    "        gallery_eval=gallery_eval,\n",
    "        test_ds=test_ds,\n",
    "        test_detections=test_detections,\n",
    "        test_reid_tf=test_reid_tf,\n",
    "        device=device,\n",
    "    )\n",
    "    results[epoch] = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T15:24:44.878220Z",
     "iopub.status.busy": "2026-01-30T15:24:44.877735Z",
     "iopub.status.idle": "2026-01-30T15:33:04.537712Z",
     "shell.execute_reply": "2026-01-30T15:33:04.535774Z",
     "shell.execute_reply.started": "2026-01-30T15:24:44.878192Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Eval] Loading checkpoint: reid_arcface_m25_epoch20.pth\n",
      "search ranking:\n",
      "  mAP = 43.66%\n",
      "  top- 1 = 82.74%\n",
      "[Eval] reid_arcface_m25_epoch20.pth  mAP=43.66% | top-1=82.74%\n"
     ]
    }
   ],
   "source": [
    "ckpt_epochs = [20]\n",
    "ckpt_paths  = [f\"reid_arcface_m25_epoch{e:02d}.pth\" for e in ckpt_epochs]\n",
    "\n",
    "results = {}\n",
    "for epoch, ckpt_path in zip(ckpt_epochs, ckpt_paths):\n",
    "    ret = evaluate_checkpoint_prw(\n",
    "        ckpt_path=ckpt_path,\n",
    "        model=model_reid_af,\n",
    "        query_ds=query_ds,\n",
    "        gallery_eval=gallery_eval,\n",
    "        test_ds=test_ds,\n",
    "        test_detections=test_detections,\n",
    "        test_reid_tf=test_reid_tf,\n",
    "        device=device,\n",
    "    )\n",
    "    results[epoch] = ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch-Hard methods\n",
    "The following methods include **negative samples** within each mini-batch, through negative mining or other techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:22:24.731703Z",
     "iopub.status.busy": "2026-02-10T10:22:24.730982Z",
     "iopub.status.idle": "2026-02-10T10:22:24.735191Z",
     "shell.execute_reply": "2026-02-10T10:22:24.734469Z",
     "shell.execute_reply.started": "2026-02-10T10:22:24.731673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Angular Triplet Loss\n",
    "It extends the standard Triplet Loss in the **angular** domain, removing the undesirable margin m that do not lie in the angular domain.\n",
    "Recall: Triplet Loss does a pairwise optimization between Anchor-Positive-Negative samples, including in a batch only **one negative** sample. On the other hand, negative mining can lead to problems, since it can be inefficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:22:28.888272Z",
     "iopub.status.busy": "2026-02-10T10:22:28.887484Z",
     "iopub.status.idle": "2026-02-10T10:22:28.902133Z",
     "shell.execute_reply": "2026-02-10T10:22:28.901480Z",
     "shell.execute_reply.started": "2026-02-10T10:22:28.888231Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# PK sampler for triplet/contrastive\n",
    "# =========================\n",
    "\n",
    "class RandomIdentitySampler(Sampler):\n",
    "    \"\"\"\n",
    "    Samples P identities, and for each identity samples K instances.\n",
    "    This makes triplet / InfoNCE actually work (positives in-batch).\n",
    "    Works with our PRWReIDDatasetCE because it returns contiguous labels. :contentReference[oaicite:3]{index=3}\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, num_identities: int, num_instances: int, seed: int = 42):\n",
    "        self.dataset = dataset\n",
    "        self.P = int(num_identities)\n",
    "        self.K = int(num_instances)\n",
    "        self.seed = int(seed)\n",
    "\n",
    "        # build label -> indices\n",
    "        self.index_dict = {}\n",
    "        for idx in range(len(dataset)):\n",
    "            # dataset[idx] returns (crop, label, pid, camid)\n",
    "            _, label, _, _ = dataset[idx]\n",
    "            lab = int(label)\n",
    "            self.index_dict.setdefault(lab, []).append(idx)\n",
    "\n",
    "        self.labels = list(self.index_dict.keys())\n",
    "        self.num_samples_per_batch = self.P * self.K\n",
    "\n",
    "        # estimate epoch length (rough but fine)\n",
    "        self.length = len(self.labels) * self.K\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __iter__(self):\n",
    "        g = np.random.RandomState(self.seed)\n",
    "\n",
    "        # shuffle identities each epoch\n",
    "        labels = self.labels.copy()\n",
    "        g.shuffle(labels)\n",
    "\n",
    "        batch = []\n",
    "        for lab in labels:\n",
    "            idxs = self.index_dict[lab]\n",
    "            if len(idxs) == 0:\n",
    "                continue\n",
    "\n",
    "            # sample K instances (with replacement if not enough)\n",
    "            if len(idxs) >= self.K:\n",
    "                chosen = g.choice(idxs, size=self.K, replace=False)\n",
    "            else:\n",
    "                chosen = g.choice(idxs, size=self.K, replace=True)\n",
    "\n",
    "            batch.extend(chosen.tolist())\n",
    "\n",
    "            if len(batch) == self.num_samples_per_batch:\n",
    "                yield from batch\n",
    "                batch = []\n",
    "\n",
    "# =========================\n",
    "# ReID model that outputs just embeddings \n",
    "# =========================\n",
    "class ReIDNetEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Same backbone+pool+fc as ReIDNet, but no classifier head.\n",
    "    Returns L2-normalized embedding (B, D), ready for angular losses. :contentReference[oaicite:4]{index=4}\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim: int = 512):\n",
    "        super().__init__()\n",
    "        backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(2048, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)\n",
    "        f = self.pool(f).flatten(1)\n",
    "        z = self.fc(f)\n",
    "        z = F.normalize(z, dim=1)  \n",
    "        return z\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Angular Triplet Loss (Batch-Hard)\n",
    "# =========================\n",
    "class AngularTripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Batch-hard angular triplet:\n",
    "      L_i = relu( theta_ap - theta_an + margin )\n",
    "    where theta = arccos(cosine_similarity), in radians.\n",
    "\n",
    "    Requirements:\n",
    "      - embeddings are L2-normalized\n",
    "      - batch should contain >=2 samples for some labels (use PK sampler!)\n",
    "    \"\"\"\n",
    "    def __init__(self, margin_rad: float = 0.35, eps: float = 1e-7):\n",
    "        super().__init__()\n",
    "        self.margin = float(margin_rad)\n",
    "        self.eps = float(eps)\n",
    "\n",
    "    def forward(self, emb: torch.Tensor, labels: torch.Tensor):\n",
    "        \"\"\"\n",
    "        emb: (B,D) normalized\n",
    "        labels: (B,) long\n",
    "        \"\"\"\n",
    "        device = emb.device\n",
    "        B = emb.size(0)\n",
    "        if B < 2:\n",
    "            return emb.new_tensor(0.0)\n",
    "\n",
    "        labels = labels.view(-1)\n",
    "        # cosine similarity matrix (B,B)\n",
    "        cos = emb @ emb.t()\n",
    "        cos = cos.clamp(-1.0 + self.eps, 1.0 - self.eps)\n",
    "\n",
    "        # angle matrix (B,B)\n",
    "        theta = torch.acos(cos)  # in [0, pi]\n",
    "\n",
    "        # masks\n",
    "        same = labels.unsqueeze(0) == labels.unsqueeze(1)      # positives (incl diag)\n",
    "        diff = ~same                                           # negatives\n",
    "\n",
    "        # exclude self from positives\n",
    "        eye = torch.eye(B, dtype=torch.bool, device=device)\n",
    "        pos_mask = same & ~eye\n",
    "        neg_mask = diff\n",
    "\n",
    "        # We want:\n",
    "        #   hardest positive = MAX angle among positives\n",
    "        #   hardest negative = MIN angle among negatives\n",
    "        # For anchors with no positives in-batch, we skip them.\n",
    "\n",
    "        # set invalid entries to -inf / +inf appropriately\n",
    "        theta_pos = theta.masked_fill(~pos_mask, float(\"-inf\"))\n",
    "        theta_neg = theta.masked_fill(~neg_mask, float(\"inf\"))\n",
    "\n",
    "        hardest_pos, _ = theta_pos.max(dim=1)  # (B,)\n",
    "        hardest_neg, _ = theta_neg.min(dim=1)  # (B,)\n",
    "\n",
    "        valid = (hardest_pos > float(\"-inf\")) & (hardest_neg < float(\"inf\"))\n",
    "        if valid.sum().item() == 0:\n",
    "            # no usable triplets in this batch\n",
    "            return emb.new_tensor(0.0)\n",
    "\n",
    "        diff = hardest_pos[valid] - hardest_neg[valid] + self.margin\n",
    "        loss = F.softplus(diff)  # no relu, stable for large margins\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T09:49:21.486389Z",
     "iopub.status.busy": "2026-02-08T09:49:21.486083Z",
     "iopub.status.idle": "2026-02-08T09:52:54.291205Z",
     "shell.execute_reply": "2026-02-08T09:52:54.290493Z",
     "shell.execute_reply.started": "2026-02-08T09:49:21.486361Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---- build triplet loader ----\n",
    "# Choose P and K so that P*K == batch_size\n",
    "P = 16 # num IDs\n",
    "K = 4  # imgs x ID\n",
    "batch_size = P * K\n",
    "\n",
    "sampler = RandomIdentitySampler(train_reid_ds, num_identities=P, num_instances=K, seed=seed)\n",
    "\n",
    "train_reid_loader_triplet = torch.utils.data.DataLoader(\n",
    "    train_reid_ds,\n",
    "    batch_size=batch_size,\n",
    "    sampler=sampler,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:06:56.350576Z",
     "iopub.status.busy": "2026-01-30T17:06:56.350252Z",
     "iopub.status.idle": "2026-01-30T17:12:23.653587Z",
     "shell.execute_reply": "2026-01-30T17:12:23.652800Z",
     "shell.execute_reply.started": "2026-01-30T17:06:56.350549Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT] Loaded 320/320 params from ArcFace checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/3833608589.py:56: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>step</td><td></td></tr><tr><td>train/emb_norm_mean</td><td></td></tr><tr><td>train/epoch_loss_avg</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/lr</td><td></td></tr><tr><td>train/neg_angle_mean</td><td></td></tr><tr><td>train/pos_angle_mean</td><td></td></tr><tr><td>train/valid_anchor_frac</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>step</td><td>599</td></tr><tr><td>train/emb_norm_mean</td><td>1</td></tr><tr><td>train/epoch_loss_avg</td><td>0.41174</td></tr><tr><td>train/loss</td><td>0.39698</td></tr><tr><td>train/lr</td><td>1e-05</td></tr><tr><td>train/neg_angle_mean</td><td>1.27046</td></tr><tr><td>train/pos_angle_mean</td><td>1.18111</td></tr><tr><td>train/valid_anchor_frac</td><td>1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">reid_r50_angtriplet_P16K4_1769791931</strong> at: <a href='https://wandb.ai/unibo-ai/person%20re-id/runs/satf3way' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id/runs/satf3way</a><br> View project at: <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260130_165211-satf3way/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20260130_170657-25zkaj3t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unibo-ai/person%20re-id/runs/25zkaj3t' target=\"_blank\">reid_r50_angtriplet_P16K4_1769792817</a></strong> to <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unibo-ai/person%20re-id/runs/25zkaj3t' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id/runs/25zkaj3t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 00] ang-triplet loss=0.0189\n",
      "[Epoch 01] ang-triplet loss=0.0179\n",
      "[Epoch 02] ang-triplet loss=0.0192\n",
      "[Epoch 03] ang-triplet loss=0.0205\n",
      "[Epoch 04] ang-triplet loss=0.0203\n",
      "[CKPT] Saved reid_angtriplet_epoch05.pth\n",
      "[Epoch 05] ang-triplet loss=0.0167\n",
      "[Epoch 06] ang-triplet loss=0.0178\n",
      "[Epoch 07] ang-triplet loss=0.0183\n",
      "[Epoch 08] ang-triplet loss=0.0178\n",
      "[Epoch 09] ang-triplet loss=0.0196\n",
      "[CKPT] Saved reid_angtriplet_epoch10.pth\n",
      "[Epoch 10] ang-triplet loss=0.0182\n",
      "[Epoch 11] ang-triplet loss=0.0188\n",
      "[Epoch 12] ang-triplet loss=0.0191\n",
      "[Epoch 13] ang-triplet loss=0.0178\n",
      "[Epoch 14] ang-triplet loss=0.0171\n",
      "[CKPT] Saved reid_angtriplet_epoch15.pth\n",
      "[Epoch 15] ang-triplet loss=0.0168\n",
      "[Epoch 16] ang-triplet loss=0.0183\n",
      "[Epoch 17] ang-triplet loss=0.0192\n",
      "[Epoch 18] ang-triplet loss=0.0192\n",
      "[Epoch 19] ang-triplet loss=0.0189\n",
      "[CKPT] Saved reid_angtriplet_epoch20.pth\n",
      "[FINAL] Saved reid_angtriplet_final.pth\n"
     ]
    }
   ],
   "source": [
    "# ==================\n",
    "# Training loop \n",
    "# ==================\n",
    "\n",
    "# ---- model ----\n",
    "model_reid_trip = ReIDNetEmbed(emb_dim=emb_dim).to(device)\n",
    "\n",
    "# -------------------- LOAD BACKBONE FROM ARCFACE --------------------\n",
    "arc_ckpt_path = \"reid_arcface_m25_epoch05.pth\"\n",
    "\n",
    "ckpt = torch.load(arc_ckpt_path, map_location=\"cpu\")\n",
    "arc_state = ckpt[\"model\"]\n",
    "\n",
    "model_state = model_reid_trip.state_dict()\n",
    "\n",
    "# carica SOLO i pesi compatibili (backbone + fc)\n",
    "filtered = {\n",
    "    k: v for k, v in arc_state.items()\n",
    "    if k in model_state and v.shape == model_state[k].shape\n",
    "}\n",
    "\n",
    "model_state.update(filtered)\n",
    "model_reid_trip.load_state_dict(model_state)\n",
    "\n",
    "print(f\"[INIT] Loaded {len(filtered)}/{len(model_state)} params from ArcFace checkpoint\")\n",
    "\n",
    "# ---- loss ----\n",
    "# 0.35 rad ~ 20 degrees\n",
    "triplet_criterion = AngularTripletLoss(margin_rad=0.20)\n",
    "\n",
    "# ---- optimizer ----\n",
    "optimizer = torch.optim.AdamW(model_reid_trip.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# -------------------- LR SCHEDULER: warmup (per-iter) + step drop (per-epoch) --------------------\n",
    "steps_per_epoch = len(train_reid_loader_triplet)  \n",
    "warmup_epochs = 1\n",
    "warmup_steps = warmup_epochs * steps_per_epoch\n",
    "\n",
    "def warmup_lambda(step: int) -> float:\n",
    "    if step < warmup_steps:\n",
    "        return (step + 1) / max(1, warmup_steps)\n",
    "    return 1.0\n",
    "\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
    "\n",
    "milestones = [16]\n",
    "gamma = 0.1\n",
    "step_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "\n",
    "# ---- wandb ----\n",
    "n_epochs = 20\n",
    "run = wandb.init(\n",
    "    entity=\"unibo-ai\",\n",
    "    project=\"person re-id\",\n",
    "    config={\n",
    "        \"seed\": seed,\n",
    "        \"dataset\": \"PRW\",\n",
    "        \"architecture\": \"ReIDNetEmbed\",\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"epochs\": n_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"sampler\": f\"RandomIdentitySampler(P={P},K={K})\",\n",
    "        \"loss\": \"AngularTripletLoss(batch-hard)\",\n",
    "        \"margin_rad\": 0.35,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"learning_rate\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"amp\": use_amp,\n",
    "    },\n",
    "    name=f\"reid_r50_angtriplet_P{P}K{K}_{int(time.time())}\",\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "@torch.no_grad()\n",
    "def angular_batch_stats(emb: torch.Tensor, labels: torch.Tensor, eps: float = 1e-7):\n",
    "    \"\"\"\n",
    "    emb: (B,D) normalized\n",
    "    labels: (B,)\n",
    "    Returns: pos_angle_mean, neg_angle_mean, valid_anchor_frac\n",
    "    \"\"\"\n",
    "    B = emb.size(0)\n",
    "    if B < 2:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    cos = (emb @ emb.t()).clamp(-1.0 + eps, 1.0 - eps)\n",
    "    theta = torch.acos(cos)\n",
    "\n",
    "    same = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
    "    eye = torch.eye(B, dtype=torch.bool, device=emb.device)\n",
    "\n",
    "    pos_mask = same & ~eye\n",
    "    neg_mask = ~same\n",
    "\n",
    "    theta_pos = theta.masked_select(pos_mask)\n",
    "    theta_neg = theta.masked_select(neg_mask)\n",
    "\n",
    "    # batch-hard valid anchors fraction\n",
    "    theta_pos_mat = theta.masked_fill(~pos_mask, float(\"-inf\"))\n",
    "    theta_neg_mat = theta.masked_fill(~neg_mask, float(\"inf\"))\n",
    "    hardest_pos = theta_pos_mat.max(dim=1).values\n",
    "    hardest_neg = theta_neg_mat.min(dim=1).values\n",
    "    valid = (hardest_pos > float(\"-inf\")) & (hardest_neg < float(\"inf\"))\n",
    "    valid_frac = valid.float().mean().item()\n",
    "\n",
    "    pos_mean = theta_pos.mean().item() if theta_pos.numel() else 0.0\n",
    "    neg_mean = theta_neg.mean().item() if theta_neg.numel() else 0.0\n",
    "    return pos_mean, neg_mean, valid_frac\n",
    "\n",
    "# -------------------- TRAIN LOOP (Triplet) --------------------\n",
    "global_step = 0\n",
    "save_every = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model_reid_trip.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    epoch_had_optimizer_step = False  \n",
    "\n",
    "    for crops, labels, pid, camid in train_reid_loader_triplet:\n",
    "        crops = crops.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True).view(-1)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "            emb = model_reid_trip(crops)\n",
    "            loss = triplet_criterion(emb, labels)\n",
    "\n",
    "        prev_scale = scaler.get_scale()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        new_scale = scaler.get_scale()\n",
    "\n",
    "        optimizer_was_stepped = (new_scale >= prev_scale)\n",
    "\n",
    "        if optimizer_was_stepped:\n",
    "            epoch_had_optimizer_step = True \n",
    "\n",
    "            # warmup per-iter ONLY when optimizer stepped\n",
    "            if global_step < warmup_steps:\n",
    "                warmup_scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss_val = float(loss.item())\n",
    "            emb_norm = emb.norm(dim=1).mean().item()\n",
    "            lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "            pos_ang, neg_ang, valid_frac = angular_batch_stats(emb, labels)\n",
    "\n",
    "        running_loss += loss_val\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train/loss\": loss_val,\n",
    "                \"train/emb_norm_mean\": emb_norm,\n",
    "                \"train/lr\": lr_now,\n",
    "                \"train/pos_angle_mean\": pos_ang,\n",
    "                \"train/neg_angle_mean\": neg_ang,\n",
    "                \"train/valid_anchor_frac\": valid_frac,\n",
    "                \"epoch\": epoch,\n",
    "                \"step\": global_step,\n",
    "            },\n",
    "            step=global_step\n",
    "        )\n",
    "        global_step += 1\n",
    "\n",
    "    epoch_loss = running_loss / max(1, len(train_reid_loader_triplet))\n",
    "    wandb.log({\"train/epoch_loss_avg\": epoch_loss, \"epoch\": epoch}, step=global_step)\n",
    "    print(f\"[Epoch {epoch:02d}] ang-triplet loss={epoch_loss:.4f}\")\n",
    "\n",
    "    # per-epoch step decay: if optimizer actually stepped during this epoch\n",
    "    if epoch_had_optimizer_step and ((epoch + 1) > warmup_epochs):\n",
    "        step_scheduler.step()\n",
    "\n",
    "    # -------------------- SAVE CHECKPOINTS --------------------\n",
    "    if ((epoch + 1) % save_every) == 0:\n",
    "        ckpt_path = f\"reid_angtriplet_epoch{epoch+1:02d}.pth\"\n",
    "        ckpt = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"global_step\": global_step,\n",
    "            \"model\": model_reid_trip.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"warmup_scheduler\": warmup_scheduler.state_dict(),\n",
    "            \"step_scheduler\": step_scheduler.state_dict(),\n",
    "            \"scaler\": scaler.state_dict() if use_amp else None,\n",
    "            \"emb_dim\": emb_dim,\n",
    "            \"pid2label\": train_reid_ds.pid2label,\n",
    "            \"config\": {\n",
    "                \"seed\": seed,\n",
    "                \"lr\": lr,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"n_epochs\": n_epochs,\n",
    "                \"use_amp\": use_amp,\n",
    "                \"save_every\": save_every,\n",
    "                \"warmup_epochs\": warmup_epochs,\n",
    "                \"milestones\": milestones,\n",
    "                \"gamma\": gamma,\n",
    "                \"loss\": \"AngularTripletLoss(batch-hard)\",\n",
    "                \"margin_rad\": getattr(triplet_criterion, \"margin\", None),\n",
    "                \"sampler\": f\"RandomIdentitySampler(P={P},K={K})\",\n",
    "            },\n",
    "        }\n",
    "        torch.save(ckpt, ckpt_path)\n",
    "\n",
    "        artifact = wandb.Artifact(f\"reid_angtriplet_epoch{epoch+1:02d}\", type=\"model\")\n",
    "        artifact.add_file(ckpt_path)\n",
    "        wandb.log_artifact(artifact)\n",
    "        print(f\"[CKPT] Saved {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:16:48.631535Z",
     "iopub.status.busy": "2026-01-30T17:16:48.631219Z",
     "iopub.status.idle": "2026-01-30T17:49:55.535574Z",
     "shell.execute_reply": "2026-01-30T17:49:55.534870Z",
     "shell.execute_reply.started": "2026-01-30T17:16:48.631507Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Eval] Loading checkpoint: reid_angtriplet_epoch05.pth\n",
      "search ranking:\n",
      "  mAP = 43.35%\n",
      "  top- 1 = 83.28%\n",
      "[Eval] reid_angtriplet_epoch05.pth  mAP=43.35% | top-1=83.28%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_angtriplet_epoch20.pth\n",
      "search ranking:\n",
      "  mAP = 43.40%\n",
      "  top- 1 = 83.33%\n",
      "[Eval] reid_angtriplet_epoch20.pth  mAP=43.40% | top-1=83.33%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_angtriplet_epoch10.pth\n",
      "search ranking:\n",
      "  mAP = 43.31%\n",
      "  top- 1 = 83.33%\n",
      "[Eval] reid_angtriplet_epoch10.pth  mAP=43.31% | top-1=83.33%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_angtriplet_epoch15.pth\n",
      "search ranking:\n",
      "  mAP = 43.42%\n",
      "  top- 1 = 83.33%\n",
      "[Eval] reid_angtriplet_epoch15.pth  mAP=43.42% | top-1=83.33%\n"
     ]
    }
   ],
   "source": [
    "ckpt_epochs = [5, 20, 10, 15]\n",
    "ckpt_paths  = [f\"reid_angtriplet_epoch{e:02d}.pth\" for e in ckpt_epochs]\n",
    "\n",
    "results = {}\n",
    "for epoch, ckpt_path in zip(ckpt_epochs, ckpt_paths):\n",
    "    ret = evaluate_checkpoint_prw(\n",
    "        ckpt_path=ckpt_path,\n",
    "        model=model_reid_trip,\n",
    "        query_ds=query_ds,\n",
    "        gallery_eval=gallery_eval,\n",
    "        test_ds=test_ds,\n",
    "        test_detections=test_detections,\n",
    "        test_reid_tf=test_reid_tf,\n",
    "        device=device,\n",
    "    )\n",
    "    results[epoch] = ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CosineAnnealing Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:05:01.980214Z",
     "iopub.status.busy": "2026-01-30T18:05:01.979818Z",
     "iopub.status.idle": "2026-01-30T18:10:39.950917Z",
     "shell.execute_reply": "2026-01-30T18:10:39.949739Z",
     "shell.execute_reply.started": "2026-01-30T18:05:01.980181Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT] Loaded 320/320 params from ArcFace checkpoint: reid_arcface_m25_epoch05.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">reid_r50_angtriplet_cosine_warmup_1769796055</strong> at: <a href='https://wandb.ai/unibo-ai/person%20re-id/runs/u5w6520p' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id/runs/u5w6520p</a><br> View project at: <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260130_180055-u5w6520p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20260130_180502-xio91tef</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unibo-ai/person%20re-id/runs/xio91tef' target=\"_blank\">reid_r50_angtriplet_cosine_warmup_1769796302</a></strong> to <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unibo-ai/person%20re-id/runs/xio91tef' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id/runs/xio91tef</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 00] ang-triplet loss=0.0191\n",
      "[Epoch 01] ang-triplet loss=0.0171\n",
      "[Epoch 02] ang-triplet loss=0.0202\n",
      "[Epoch 03] ang-triplet loss=0.0187\n",
      "[Epoch 04] ang-triplet loss=0.0182\n",
      "[CKPT] Saved reid_angtriplet_cosine_epoch05.pth\n",
      "[Epoch 05] ang-triplet loss=0.0208\n",
      "[Epoch 06] ang-triplet loss=0.0198\n",
      "[Epoch 07] ang-triplet loss=0.0200\n",
      "[Epoch 08] ang-triplet loss=0.0162\n",
      "[Epoch 09] ang-triplet loss=0.0191\n",
      "[CKPT] Saved reid_angtriplet_cosine_epoch10.pth\n",
      "[Epoch 10] ang-triplet loss=0.0194\n",
      "[Epoch 11] ang-triplet loss=0.0195\n",
      "[Epoch 12] ang-triplet loss=0.0198\n",
      "[Epoch 13] ang-triplet loss=0.0182\n",
      "[Epoch 14] ang-triplet loss=0.0165\n",
      "[CKPT] Saved reid_angtriplet_cosine_epoch15.pth\n",
      "[Epoch 15] ang-triplet loss=0.0180\n",
      "[Epoch 16] ang-triplet loss=0.0195\n",
      "[Epoch 17] ang-triplet loss=0.0184\n",
      "[Epoch 18] ang-triplet loss=0.0206\n",
      "[Epoch 19] ang-triplet loss=0.0194\n",
      "[CKPT] Saved reid_angtriplet_cosine_epoch20.pth\n",
      "[FINAL] Saved reid_angtriplet_cosine_final.pth\n"
     ]
    }
   ],
   "source": [
    "# ==================\n",
    "# Training loop (Angular Triplet) + Warmup (manual per-iter) + CosineAnnealing (per-epoch)\n",
    "# -------------------- MODEL --------------------\n",
    "model_reid_trip = ReIDNetEmbed(emb_dim=emb_dim).to(device)\n",
    "\n",
    "# -------------------- LOAD BACKBONE FROM ARCFACE --------------------\n",
    "arc_ckpt_path = \"reid_arcface_m25_epoch05.pth\"\n",
    "ckpt = torch.load(arc_ckpt_path, map_location=\"cpu\")\n",
    "arc_state = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else ckpt\n",
    "\n",
    "model_state = model_reid_trip.state_dict()\n",
    "filtered = {k: v for k, v in arc_state.items() if k in model_state and v.shape == model_state[k].shape}\n",
    "model_state.update(filtered)\n",
    "model_reid_trip.load_state_dict(model_state)\n",
    "print(f\"[INIT] Loaded {len(filtered)}/{len(model_state)} params from ArcFace checkpoint: {arc_ckpt_path}\")\n",
    "\n",
    "# -------------------- LOSS --------------------\n",
    "triplet_criterion = AngularTripletLoss(margin_rad=0.20)\n",
    "\n",
    "# -------------------- OPTIMIZER --------------------\n",
    "base_lr = 3e-4\n",
    "weight_decay = 1e-4\n",
    "optimizer = torch.optim.AdamW(model_reid_trip.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "\n",
    "# -------------------- AMP --------------------\n",
    "scaler = GradScaler(\"cuda\", enabled=use_amp)\n",
    "\n",
    "# -------------------- DATALOADER (assumed already built) --------------------\n",
    "# train_reid_loader_triplet must exist\n",
    "steps_per_epoch = len(train_reid_loader_triplet)\n",
    "\n",
    "# -------------------- LR SCHEDULE: manual warmup per-iter + cosine per-epoch --------------------\n",
    "n_epochs = 20\n",
    "warmup_epochs = 1\n",
    "warmup_steps = warmup_epochs * steps_per_epoch\n",
    "\n",
    "eta_min = 1e-6\n",
    "cosine_T_max = max(1, n_epochs - warmup_epochs)\n",
    "cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=cosine_T_max, eta_min=eta_min\n",
    ")\n",
    "\n",
    "# Store the base LR for each param group to compute warmup scaling\n",
    "for pg in optimizer.param_groups:\n",
    "    pg.setdefault(\"initial_lr\", pg[\"lr\"])\n",
    "    pg[\"lr\"] = eta_min  # start low; warmup will ramp up after optimizer steps\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def angular_batch_stats(emb: torch.Tensor, labels: torch.Tensor, eps: float = 1e-7):\n",
    "    B = emb.size(0)\n",
    "    if B < 2:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    cos = (emb @ emb.t()).clamp(-1.0 + eps, 1.0 - eps)\n",
    "    theta = torch.acos(cos)\n",
    "\n",
    "    same = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
    "    eye = torch.eye(B, dtype=torch.bool, device=emb.device)\n",
    "    pos_mask = same & ~eye\n",
    "    neg_mask = ~same\n",
    "\n",
    "    theta_pos = theta.masked_select(pos_mask)\n",
    "    theta_neg = theta.masked_select(neg_mask)\n",
    "\n",
    "    theta_pos_mat = theta.masked_fill(~pos_mask, float(\"-inf\"))\n",
    "    theta_neg_mat = theta.masked_fill(~neg_mask, float(\"inf\"))\n",
    "    hardest_pos = theta_pos_mat.max(dim=1).values\n",
    "    hardest_neg = theta_neg_mat.min(dim=1).values\n",
    "    valid = (hardest_pos > float(\"-inf\")) & (hardest_neg < float(\"inf\"))\n",
    "    valid_frac = valid.float().mean().item()\n",
    "\n",
    "    pos_mean = theta_pos.mean().item() if theta_pos.numel() else 0.0\n",
    "    neg_mean = theta_neg.mean().item() if theta_neg.numel() else 0.0\n",
    "    return pos_mean, neg_mean, valid_frac\n",
    "\n",
    "\n",
    "# -------------------- WANDB INIT --------------------\n",
    "run = wandb.init(\n",
    "    entity=\"unibo-ai\",\n",
    "    project=\"person re-id\",\n",
    "    config={\n",
    "        \"seed\": seed,\n",
    "        \"dataset\": \"PRW\",\n",
    "        \"architecture\": \"ReIDNetEmbed\",\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"epochs\": n_epochs,\n",
    "        \"batch_size\": getattr(train_reid_loader_triplet, \"batch_size\", None),\n",
    "        \"loss\": \"AngularTripletLoss(batch-hard)\",\n",
    "        \"margin_rad\": 0.20,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"learning_rate\": base_lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"scheduler\": f\"manual-warmup({warmup_epochs} epoch, per-iter) + CosineAnnealingLR(T_max={cosine_T_max}, eta_min={eta_min})\",\n",
    "        \"warmup_epochs\": warmup_epochs,\n",
    "        \"eta_min\": eta_min,\n",
    "        \"amp\": use_amp,\n",
    "        \"arcface_init_ckpt\": arc_ckpt_path,\n",
    "        \"save_every_epochs\": 5,\n",
    "    },\n",
    "    name=f\"reid_r50_angtriplet_cosine_warmup_{int(time.time())}\",\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "# -------------------- TRAIN LOOP --------------------\n",
    "global_step = 0\n",
    "save_every = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model_reid_trip.train()\n",
    "    running_loss = 0.0\n",
    "    epoch_had_optimizer_step = False\n",
    "\n",
    "    for crops, labels, pid, camid in train_reid_loader_triplet:\n",
    "        crops = crops.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True).view(-1)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "            emb = model_reid_trip(crops)\n",
    "            loss = triplet_criterion(emb, labels)\n",
    "\n",
    "        prev_scale = scaler.get_scale()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)  # may be skipped on overflow\n",
    "        scaler.update()\n",
    "        new_scale = scaler.get_scale()\n",
    "\n",
    "        optimizer_was_stepped = (new_scale >= prev_scale)\n",
    "        if optimizer_was_stepped:\n",
    "            epoch_had_optimizer_step = True\n",
    "\n",
    "            # --------- MANUAL WARMUP (NO lr_scheduler.step -> no order warning) ---------\n",
    "            if global_step < warmup_steps:\n",
    "                warm = (global_step + 1) / max(1, warmup_steps)  # in (0,1]\n",
    "                for pg in optimizer.param_groups:\n",
    "                    target = pg.get(\"initial_lr\", base_lr)\n",
    "                    pg[\"lr\"] = eta_min + warm * (target - eta_min)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss_val = float(loss.item())\n",
    "            emb_norm = emb.norm(dim=1).mean().item()\n",
    "            lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "            pos_ang, neg_ang, valid_frac = angular_batch_stats(emb, labels)\n",
    "\n",
    "        running_loss += loss_val\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train/loss\": loss_val,\n",
    "                \"train/epoch\": epoch,\n",
    "                \"train/step\": global_step,\n",
    "                \"train/lr\": lr_now,\n",
    "                \"train/emb_norm_mean\": emb_norm,\n",
    "                \"train/pos_angle_mean\": pos_ang,\n",
    "                \"train/neg_angle_mean\": neg_ang,\n",
    "                \"train/valid_anchor_frac\": valid_frac,\n",
    "            },\n",
    "            step=global_step\n",
    "        )\n",
    "        global_step += 1\n",
    "\n",
    "    epoch_loss = running_loss / max(1, len(train_reid_loader_triplet))\n",
    "    wandb.log({\"train/epoch_loss_avg\": epoch_loss, \"epoch\": epoch}, step=global_step)\n",
    "    print(f\"[Epoch {epoch:02d}] ang-triplet loss={epoch_loss:.4f}\")\n",
    "\n",
    "    # --------- COSINE STEP (per-epoch) ---------\n",
    "    # Only after warmup epochs, and only if optimizer stepped at least once in epoch\n",
    "    if epoch_had_optimizer_step and (epoch + 1) > warmup_epochs:\n",
    "        cosine_scheduler.step()\n",
    "\n",
    "    # -------------------- SAVE CHECKPOINTS --------------------\n",
    "    if ((epoch + 1) % save_every) == 0:\n",
    "        ckpt_path = f\"reid_angtriplet_cosine_epoch{epoch+1:02d}.pth\"\n",
    "        ckpt_out = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"global_step\": global_step,\n",
    "            \"model\": model_reid_trip.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"cosine_scheduler\": cosine_scheduler.state_dict(),\n",
    "            \"scaler\": scaler.state_dict() if use_amp else None,\n",
    "            \"emb_dim\": emb_dim,\n",
    "            \"pid2label\": train_reid_ds.pid2label,\n",
    "            \"config\": {\n",
    "                \"seed\": seed,\n",
    "                \"base_lr\": base_lr,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"n_epochs\": n_epochs,\n",
    "                \"use_amp\": use_amp,\n",
    "                \"save_every\": save_every,\n",
    "                \"warmup_epochs\": warmup_epochs,\n",
    "                \"warmup_steps\": warmup_steps,\n",
    "                \"eta_min\": eta_min,\n",
    "                \"cosine_T_max\": cosine_T_max,\n",
    "                \"loss\": \"AngularTripletLoss(batch-hard)\",\n",
    "                \"margin_rad\": getattr(triplet_criterion, \"margin\", None),\n",
    "                \"arcface_init_ckpt\": arc_ckpt_path,\n",
    "            },\n",
    "        }\n",
    "        torch.save(ckpt_out, ckpt_path)\n",
    "\n",
    "        artifact = wandb.Artifact(f\"reid_angtriplet_cosine_epoch{epoch+1:02d}\", type=\"model\")\n",
    "        artifact.add_file(ckpt_path)\n",
    "        wandb.log_artifact(artifact)\n",
    "        print(f\"[CKPT] Saved {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:11:49.340099Z",
     "iopub.status.busy": "2026-01-30T18:11:49.339797Z",
     "iopub.status.idle": "2026-01-30T18:45:06.536084Z",
     "shell.execute_reply": "2026-01-30T18:45:06.535320Z",
     "shell.execute_reply.started": "2026-01-30T18:11:49.340073Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Eval] Loading checkpoint: reid_angtriplet_cosine_epoch05.pth\n",
      "search ranking:\n",
      "  mAP = 43.37%\n",
      "  top- 1 = 83.52%\n",
      "[Eval] reid_angtriplet_cosine_epoch05.pth  mAP=43.37% | top-1=83.52%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_angtriplet_cosine_epoch20.pth\n",
      "search ranking:\n",
      "  mAP = 43.36%\n",
      "  top- 1 = 83.18%\n",
      "[Eval] reid_angtriplet_cosine_epoch20.pth  mAP=43.36% | top-1=83.18%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_angtriplet_cosine_epoch10.pth\n",
      "search ranking:\n",
      "  mAP = 43.38%\n",
      "  top- 1 = 83.42%\n",
      "[Eval] reid_angtriplet_cosine_epoch10.pth  mAP=43.38% | top-1=83.42%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_angtriplet_cosine_epoch15.pth\n",
      "search ranking:\n",
      "  mAP = 43.36%\n",
      "  top- 1 = 83.33%\n",
      "[Eval] reid_angtriplet_cosine_epoch15.pth  mAP=43.36% | top-1=83.33%\n"
     ]
    }
   ],
   "source": [
    "ckpt_epochs = [5, 20, 10, 15]\n",
    "ckpt_paths  = [f\"reid_angtriplet_cosine_epoch{e:02d}.pth\" for e in ckpt_epochs]\n",
    "\n",
    "results = {}\n",
    "for epoch, ckpt_path in zip(ckpt_epochs, ckpt_paths):\n",
    "    ret = evaluate_checkpoint_prw(\n",
    "        ckpt_path=ckpt_path,\n",
    "        model=model_reid_trip,\n",
    "        query_ds=query_ds,\n",
    "        gallery_eval=gallery_eval,\n",
    "        test_ds=test_ds,\n",
    "        test_detections=test_detections,\n",
    "        test_reid_tf=test_reid_tf,\n",
    "        device=device,\n",
    "    )\n",
    "    results[epoch] = ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InfoNCE / nxTent\n",
    "In the InfoNCE loss we forget about mining: we include **multiple negative** samples in a mini-batch, in order to optimize at once distance from multiple negatives. We do that considering the **logsumexp** instead of the max operator, actually considering all the negative template classes at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:22:55.113105Z",
     "iopub.status.busy": "2026-02-10T10:22:55.112553Z",
     "iopub.status.idle": "2026-02-10T10:22:55.128151Z",
     "shell.execute_reply": "2026-02-10T10:22:55.127504Z",
     "shell.execute_reply.started": "2026-02-10T10:22:55.113077Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Supervised InfoNCE training\n",
    "# - FAST PK sampler (no __getitem__ calls)\n",
    "# - AMP\n",
    "# - warmup per-step\n",
    "# - cosine per-epoch\n",
    "# - checkpoints\n",
    "# ============================\n",
    "\n",
    "# ----------------------------\n",
    "# 0) CONFIG\n",
    "# ----------------------------\n",
    "P = 16\n",
    "K = 4\n",
    "batch_size = P * K\n",
    "\n",
    "num_workers = 2\n",
    "pin_memory = True\n",
    "\n",
    "base_lr = 3e-4\n",
    "weight_decay = 1e-4\n",
    "\n",
    "warmup_epochs = 1\n",
    "eta_min = 1e-6\n",
    "\n",
    "save_every = 5\n",
    "out_dir = \".\"\n",
    "\n",
    "use_wandb = True\n",
    "wandb_entity = \"unibo-ai\"\n",
    "wandb_project = \"person re-id\"\n",
    "\n",
    "arc_init_ckpt_path = \"/kaggle/input/arcface-5-epochs/reid_arcface_m25_epoch05.pth\"  # set None to disable\n",
    "temperature = 0.07\n",
    "\n",
    "run_name = f\"reid_supinfonce_{int(time.time())}\"\n",
    "\n",
    "# ----------------------\n",
    "# 1) FAST PK SAMPLER\n",
    "# ----------------------\n",
    "class RandomIdentitySamplerFast(Sampler):\n",
    "    \"\"\"\n",
    "    Samples P identities, K instances each -> batch = P*K.\n",
    "    Reads labels from dataset.samples + dataset.pid2label (NO image loading).\n",
    "    Also changes sampling each epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, num_identities: int, num_instances: int, seed: int = 42):\n",
    "        self.dataset = dataset\n",
    "        self.P = int(num_identities)\n",
    "        self.K = int(num_instances)\n",
    "        self.seed = int(seed)\n",
    "        self.epoch = 0\n",
    "\n",
    "        # Build label -> indices using dataset.samples (fast)\n",
    "        # label = dataset.pid2label[ pid ]\n",
    "        self.index_dict = {}\n",
    "        for idx, s in enumerate(dataset.samples):\n",
    "            pid = int(s[\"pid\"])\n",
    "            lab = int(dataset.pid2label[pid])\n",
    "            self.index_dict.setdefault(lab, []).append(idx)\n",
    "\n",
    "        self.labels = list(self.index_dict.keys())\n",
    "        self.num_samples_per_batch = self.P * self.K\n",
    "\n",
    "        # Rough epoch length (doesn't need to be exact)\n",
    "        # Number of full batches we can make if each id contributes K samples:\n",
    "        self.length = (len(self.labels) // self.P) * self.num_samples_per_batch\n",
    "\n",
    "    def set_epoch(self, epoch: int):\n",
    "        self.epoch = int(epoch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __iter__(self):\n",
    "        g = np.random.RandomState(self.seed + self.epoch)\n",
    "\n",
    "        # shuffle identities\n",
    "        labels = self.labels.copy()\n",
    "        g.shuffle(labels)\n",
    "\n",
    "        batch = []\n",
    "        # take identities in chunks of P\n",
    "        for start in range(0, len(labels) - self.P + 1, self.P):\n",
    "            chosen_ids = labels[start:start + self.P]\n",
    "\n",
    "            for lab in chosen_ids:\n",
    "                idxs = self.index_dict[lab]\n",
    "                if len(idxs) >= self.K:\n",
    "                    picked = g.choice(idxs, size=self.K, replace=False)\n",
    "                else:\n",
    "                    picked = g.choice(idxs, size=self.K, replace=True)\n",
    "                batch.extend(picked.tolist())\n",
    "\n",
    "            # yield exactly one batch (P*K)\n",
    "            yield from batch\n",
    "            batch = []\n",
    "\n",
    "# ----------------------------\n",
    "# 3) MODEL (embedding-only)\n",
    "# ----------------------------\n",
    "class ReIDNetEmbed(nn.Module):\n",
    "    def __init__(self, emb_dim: int = 512):\n",
    "        super().__init__()\n",
    "        backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(2048, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)\n",
    "        f = self.pool(f).flatten(1)\n",
    "        z = self.fc(f)\n",
    "        z = F.normalize(z, dim=1)  # already normalized\n",
    "        return z\n",
    "\n",
    "# ----------------------------\n",
    "# 4) LOSS: Supervised InfoNCE\n",
    "# ----------------------------\n",
    "class SupInfoNCELoss(nn.Module):\n",
    "    def __init__(self, temperature: float = 0.07, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.tau = float(temperature)\n",
    "        self.eps = float(eps)\n",
    "\n",
    "    def forward(self, emb: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        B = emb.size(0)\n",
    "        if B < 2:\n",
    "            return emb.sum() * 0.0\n",
    "\n",
    "        # emb is expected float32 here\n",
    "        emb = F.normalize(emb, dim=1)\n",
    "        labels = labels.view(-1).long()\n",
    "        device = emb.device\n",
    "\n",
    "        logits = (emb @ emb.t()) / self.tau  # float32\n",
    "        eye = torch.eye(B, dtype=torch.bool, device=device)\n",
    "\n",
    "        # safe big negative in fp32\n",
    "        logits = logits.masked_fill(eye, -1e9)\n",
    "\n",
    "        lab = labels.view(-1, 1)\n",
    "        pos_mask = (lab == lab.t()) & (~eye)\n",
    "        pos_count = pos_mask.sum(dim=1)\n",
    "\n",
    "        valid = pos_count > 0\n",
    "        if valid.sum() == 0:\n",
    "            return emb.sum() * 0.0\n",
    "\n",
    "        log_prob = F.log_softmax(logits, dim=1)\n",
    "        pos_log_prob_sum = (log_prob * pos_mask.float()).sum(dim=1)\n",
    "        loss_i = -pos_log_prob_sum / (pos_count.float() + self.eps)\n",
    "\n",
    "        return loss_i[valid].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T15:30:08.701153Z",
     "iopub.status.busy": "2026-02-01T15:30:08.700104Z",
     "iopub.status.idle": "2026-02-01T15:35:58.107457Z",
     "shell.execute_reply": "2026-02-01T15:35:58.106601Z",
     "shell.execute_reply.started": "2026-02-01T15:30:08.701084Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT] Loaded 320/320 params from: /kaggle/input/arcface-5-epochs/reid_arcface_m25_epoch05.pth\n",
      "build_laoder\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">reid_supinfonce_1769959648</strong> at: <a href='https://wandb.ai/unibo-ai/person%20re-id/runs/jsx5olwy' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id/runs/jsx5olwy</a><br> View project at: <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260201_152728-jsx5olwy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20260201_153009-ue95co7o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unibo-ai/person%20re-id/runs/ue95co7o' target=\"_blank\">reid_supinfonce_1769959808</a></strong> to <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unibo-ai/person%20re-id/runs/ue95co7o' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id/runs/ue95co7o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting train loop..\n",
      "[Epoch 01/20] loss=1.5331 | lr=3.00e-04 | 18.1s\n",
      "[Epoch 02/20] loss=1.3834 | lr=3.00e-04 | 17.4s\n",
      "[Epoch 03/20] loss=1.3220 | lr=2.98e-04 | 17.2s\n",
      "[Epoch 04/20] loss=1.2954 | lr=2.92e-04 | 16.8s\n",
      "[Epoch 05/20] loss=1.3076 | lr=2.82e-04 | 16.8s\n",
      "[CKPT] Saved ./reid_supinfonce_epoch05.pth\n",
      "[Epoch 06/20] loss=1.2716 | lr=2.68e-04 | 17.0s\n",
      "[Epoch 07/20] loss=1.2620 | lr=2.52e-04 | 16.9s\n",
      "[Epoch 08/20] loss=1.2610 | lr=2.32e-04 | 16.5s\n",
      "[Epoch 09/20] loss=1.2692 | lr=2.11e-04 | 16.5s\n",
      "[Epoch 10/20] loss=1.2369 | lr=1.87e-04 | 16.9s\n",
      "[CKPT] Saved ./reid_supinfonce_epoch10.pth\n",
      "[Epoch 11/20] loss=1.2253 | lr=1.63e-04 | 16.9s\n",
      "[Epoch 12/20] loss=1.2150 | lr=1.38e-04 | 16.6s\n",
      "[Epoch 13/20] loss=1.2077 | lr=1.14e-04 | 16.9s\n",
      "[Epoch 14/20] loss=1.1980 | lr=9.04e-05 | 16.7s\n",
      "[Epoch 15/20] loss=1.1971 | lr=6.87e-05 | 16.8s\n",
      "[CKPT] Saved ./reid_supinfonce_epoch15.pth\n",
      "[Epoch 16/20] loss=1.1986 | lr=4.92e-05 | 16.8s\n",
      "[Epoch 17/20] loss=1.1864 | lr=3.25e-05 | 16.7s\n",
      "[Epoch 18/20] loss=1.1947 | lr=1.90e-05 | 17.0s\n",
      "[Epoch 19/20] loss=1.1849 | lr=9.10e-06 | 16.9s\n",
      "[Epoch 20/20] loss=1.1882 | lr=3.04e-06 | 16.9s\n",
      "[CKPT] Saved ./reid_supinfonce_epoch20.pth\n",
      "[FINAL] Saved ./reid_supinfonce_final.pth\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>train/epoch_loss_avg</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/lr</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train/epoch_loss_avg</td><td>1.18824</td></tr><tr><td>train/loss</td><td>1.18708</td></tr><tr><td>train/lr</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">reid_supinfonce_1769959808</strong> at: <a href='https://wandb.ai/unibo-ai/person%20re-id/runs/ue95co7o' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id/runs/ue95co7o</a><br> View project at: <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260201_153009-ue95co7o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Final checkpoint: ./reid_supinfonce_final.pth\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 5) UTILS\n",
    "# ----------------------------\n",
    "def set_lr(optimizer, lr: float):\n",
    "    for pg in optimizer.param_groups:\n",
    "        pg[\"lr\"] = lr\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    return optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "def save_ckpt(path, epoch, global_step, model, optimizer, scheduler, scaler, extra=None):\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch,\n",
    "        \"global_step\": global_step,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict() if scheduler is not None else None,\n",
    "        \"scaler\": scaler.state_dict() if scaler is not None else None,\n",
    "        \"extra\": extra or {},\n",
    "    }\n",
    "    torch.save(ckpt, path)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) BUILD LOADER\n",
    "# ----------------------------\n",
    "def build_loader(train_reid_ds):\n",
    "    sampler = RandomIdentitySamplerFast(train_reid_ds, num_identities=P, num_instances=K, seed=seed)\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        train_reid_ds,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=True,  # keep batches clean\n",
    "        persistent_workers=(num_workers > 0),\n",
    "        prefetch_factor=2 if num_workers > 0 else None,\n",
    "    )\n",
    "    return loader, sampler\n",
    "\n",
    "# ----------------------------\n",
    "# 7) TRAIN\n",
    "# ----------------------------\n",
    "def train_supinfonce(train_reid_ds, arc_init_ckpt_path=None, temperature=0.07):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    model = ReIDNetEmbed(emb_dim=emb_dim).to(device)\n",
    "\n",
    "    # Optional init from ArcFace checkpoint \n",
    "    if arc_init_ckpt_path is not None:\n",
    "        ckpt = torch.load(arc_init_ckpt_path, map_location=\"cpu\")\n",
    "        state = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else ckpt\n",
    "        ms = model.state_dict()\n",
    "        filtered = {k: v for k, v in state.items() if k in ms and v.shape == ms[k].shape}\n",
    "        ms.update(filtered)\n",
    "        model.load_state_dict(ms)\n",
    "        print(f\"[INIT] Loaded {len(filtered)}/{len(ms)} params from: {arc_init_ckpt_path}\")\n",
    "\n",
    "    print(\"build_laoder\")\n",
    "    train_loader, sampler = build_loader(train_reid_ds)\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    warmup_steps = warmup_epochs * steps_per_epoch\n",
    "\n",
    "    loss_fn = SupInfoNCELoss(temperature=temperature).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "    set_lr(optimizer, eta_min)\n",
    "\n",
    "    cosine_T_max = max(1, n_epochs - warmup_epochs)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cosine_T_max, eta_min=eta_min)\n",
    "\n",
    "    scaler = GradScaler(\"cuda\", enabled=use_amp)\n",
    "\n",
    "    # wandb\n",
    "    if use_wandb:\n",
    "        import wandb\n",
    "        run = wandb.init(\n",
    "            entity=wandb_entity,\n",
    "            project=wandb_project,\n",
    "            name=run_name,\n",
    "            config={\n",
    "                \"loss\": \"SupInfoNCE\",\n",
    "                \"temperature\": temperature,\n",
    "                \"P\": P, \"K\": K,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"lr\": base_lr,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"warmup_epochs\": warmup_epochs,\n",
    "                \"eta_min\": eta_min,\n",
    "                \"epochs\": n_epochs,\n",
    "                \"amp\": use_amp,\n",
    "                \"arc_init_ckpt\": arc_init_ckpt_path,\n",
    "            },\n",
    "            reinit=True\n",
    "        )\n",
    "    else:\n",
    "        run = None\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    print(\"starting train loop..\")\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        sampler.set_epoch(epoch)  # IMPORTANT\n",
    "\n",
    "        epoch_loss_sum = 0.0\n",
    "        t0 = time.time()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            crops = batch[0].to(device, non_blocking=True)\n",
    "            labels = batch[1].to(device, non_blocking=True).view(-1).long()\n",
    "\n",
    "            # warmup per-step\n",
    "            if global_step < warmup_steps:\n",
    "                warm = (global_step + 1) / max(1, warmup_steps)\n",
    "                lr_now = eta_min + warm * (base_lr - eta_min)\n",
    "                set_lr(optimizer, lr_now)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                emb = model(crops)               # (B,D)\n",
    "            loss = loss_fn(emb.float(), labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            loss_val = float(loss.item())\n",
    "            epoch_loss_sum += loss_val\n",
    "\n",
    "            if run is not None:\n",
    "                run.log({\"train/loss\": loss_val, \"train/lr\": get_lr(optimizer), \"epoch\": epoch}, step=global_step)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        epoch_loss = epoch_loss_sum / max(1, len(train_loader))\n",
    "        dt = time.time() - t0\n",
    "        print(f\"[Epoch {epoch+1:02d}/{n_epochs}] loss={epoch_loss:.4f} | lr={get_lr(optimizer):.2e} | {dt:.1f}s\")\n",
    "\n",
    "        if run is not None:\n",
    "            run.log({\"train/epoch_loss_avg\": epoch_loss}, step=global_step)\n",
    "\n",
    "        # cosine per-epoch AFTER warmup\n",
    "        if (epoch + 1) > warmup_epochs:\n",
    "            scheduler.step()\n",
    "\n",
    "        # save\n",
    "        if ((epoch + 1) % save_every) == 0:\n",
    "            ckpt_path = os.path.join(out_dir, f\"reid_supinfonce_epoch{epoch+1:02d}.pth\")\n",
    "            save_ckpt(\n",
    "                ckpt_path,\n",
    "                epoch=epoch + 1,\n",
    "                global_step=global_step,\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                scaler=scaler if use_amp else None,\n",
    "                extra={\n",
    "                    \"temperature\": temperature,\n",
    "                    \"P\": P, \"K\": K,\n",
    "                    \"pid2label\": getattr(train_reid_ds, \"pid2label\", None),\n",
    "                },\n",
    "            )\n",
    "            print(f\"[CKPT] Saved {ckpt_path}\")\n",
    "\n",
    "    final_path = os.path.join(out_dir, \"reid_supinfonce_final.pth\")\n",
    "    save_ckpt(\n",
    "        final_path,\n",
    "        epoch=n_epochs,\n",
    "        global_step=global_step,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        scaler=scaler if use_amp else None,\n",
    "        extra={\"temperature\": temperature, \"P\": P, \"K\": K},\n",
    "    )\n",
    "    print(f\"[FINAL] Saved {final_path}\")\n",
    "\n",
    "    if run is not None:\n",
    "        run.finish()\n",
    "\n",
    "    return final_path, model\n",
    "\n",
    "# usage\n",
    "final_ckpt = train_supinfonce(\n",
    "    train_reid_ds=train_reid_ds,\n",
    "    arc_init_ckpt_path=arc_init_ckpt_path,\n",
    "    temperature=temperature\n",
    ")\n",
    "\n",
    "print(\"Done. Final checkpoint:\", final_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T15:44:18.345919Z",
     "iopub.status.busy": "2026-02-01T15:44:18.345044Z",
     "iopub.status.idle": "2026-02-01T16:21:49.131741Z",
     "shell.execute_reply": "2026-02-01T16:21:49.131076Z",
     "shell.execute_reply.started": "2026-02-01T15:44:18.345880Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Eval] Loading checkpoint: reid_supinfonce_epoch20.pth\n",
      "search ranking:\n",
      "  mAP = 47.05%\n",
      "  top- 1 = 82.01%\n",
      "[Eval] reid_supinfonce_epoch20.pth  mAP=47.05% | top-1=82.01%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_supinfonce_epoch05.pth\n",
      "search ranking:\n",
      "  mAP = 45.89%\n",
      "  top- 1 = 81.19%\n",
      "[Eval] reid_supinfonce_epoch05.pth  mAP=45.89% | top-1=81.19%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_supinfonce_epoch10.pth\n",
      "search ranking:\n",
      "  mAP = 45.52%\n",
      "  top- 1 = 80.89%\n",
      "[Eval] reid_supinfonce_epoch10.pth  mAP=45.52% | top-1=80.89%\n",
      "\n",
      "[Eval] Loading checkpoint: reid_supinfonce_epoch15.pth\n",
      "search ranking:\n",
      "  mAP = 47.07%\n",
      "  top- 1 = 81.67%\n",
      "[Eval] reid_supinfonce_epoch15.pth  mAP=47.07% | top-1=81.67%\n"
     ]
    }
   ],
   "source": [
    "ckpt_epochs = [20, 5, 10, 15]\n",
    "ckpt_paths  = [f\"reid_supinfonce_epoch{e:02d}.pth\" for e in ckpt_epochs]\n",
    "\n",
    "#model_infonce = ReIDNetEmbed(emb_dim=emb_dim).to(device)\n",
    "\n",
    "results = {}\n",
    "for epoch, ckpt_path in zip(ckpt_epochs, ckpt_paths):\n",
    "    ret = evaluate_checkpoint_prw(\n",
    "        ckpt_path=ckpt_path,\n",
    "        model=model_infonce,\n",
    "        query_ds=query_ds,\n",
    "        gallery_eval=gallery_eval,\n",
    "        test_ds=test_ds,\n",
    "        test_detections=test_detections,\n",
    "        test_reid_tf=test_reid_tf,\n",
    "        device=device,\n",
    "    )\n",
    "    results[epoch] = ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing backbone: ConvNeXt\n",
    "Changing backbone for feature extraction of the resulting retrieved bboxes embeddings. We will test it on the ArcFace model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T10:23:20.092470Z",
     "iopub.status.busy": "2026-02-10T10:23:20.091933Z",
     "iopub.status.idle": "2026-02-10T10:23:20.100002Z",
     "shell.execute_reply": "2026-02-10T10:23:20.099278Z",
     "shell.execute_reply.started": "2026-02-10T10:23:20.092423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ReIDNetArcFaceConvNeXt(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim: int,\n",
    "        num_classes: int,\n",
    "        s: float = 30.0,\n",
    "        m: float = 0.35,\n",
    "        variant: str = \"tiny\",   # \"tiny\" | \"small\" | \"base\" | \"large\"\n",
    "        pretrained: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # ------------- pick ConvNeXt variant + weights -------------\n",
    "        variant = variant.lower()\n",
    "        if variant == \"tiny\":\n",
    "            weights = models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "            backbone = models.convnext_tiny(weights=weights)\n",
    "        elif variant == \"small\":\n",
    "            weights = models.ConvNeXt_Small_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "            backbone = models.convnext_small(weights=weights)\n",
    "        elif variant == \"base\":\n",
    "            weights = models.ConvNeXt_Base_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "            backbone = models.convnext_base(weights=weights)\n",
    "        elif variant == \"large\":\n",
    "            weights = models.ConvNeXt_Large_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "            backbone = models.convnext_large(weights=weights)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown ConvNeXt variant: {variant}\")\n",
    "\n",
    "        # ConvNeXt in torchvision: backbone.features is the conv trunk\n",
    "        self.backbone = backbone.features\n",
    "\n",
    "        # ------------- head: GAP -> fc -> normalize -> ArcFace -------------\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # robust way to get feature dimension:\n",
    "        # ConvNeXt classifier ends with Linear(in_features -> 1000)\n",
    "        feat_dim = backbone.classifier[-1].in_features\n",
    "\n",
    "        self.fc = nn.Linear(feat_dim, emb_dim)\n",
    "\n",
    "        self.cls = ArcFaceClassifier(\n",
    "            in_dim=emb_dim,\n",
    "            num_classes=num_classes,\n",
    "            s=s,\n",
    "            m=m\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        f = self.backbone(x)              # (N, C, H, W)\n",
    "        f = self.pool(f).flatten(1)       # (N, C)\n",
    "        z = self.fc(f)                    # (N, emb_dim)\n",
    "        z = F.normalize(z, dim=1)\n",
    "\n",
    "        logits = self.cls(z, labels=labels)\n",
    "        return logits, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T18:05:49.284445Z",
     "iopub.status.busy": "2026-02-03T18:05:49.283673Z",
     "iopub.status.idle": "2026-02-03T18:53:06.279638Z",
     "shell.execute_reply": "2026-02-03T18:53:06.278858Z",
     "shell.execute_reply.started": "2026-02-03T18:05:49.284412Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">reid_CXt_arcface_20e_m25_adamw_warmup_step_1770141854</strong> at: <a href='https://wandb.ai/unibo-ai/person%20re-id/runs/vt84vf7s' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id/runs/vt84vf7s</a><br> View project at: <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260203_180422-vt84vf7s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20260203_180549-9d5ofrac</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/unibo-ai/person%20re-id/runs/9d5ofrac' target=\"_blank\">reid_CXt_arcface_20e_m25_adamw_warmup_step_1770141949</a></strong> to <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/unibo-ai/person%20re-id' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/unibo-ai/person%20re-id/runs/9d5ofrac' target=\"_blank\">https://wandb.ai/unibo-ai/person%20re-id/runs/9d5ofrac</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 00] loss=10.3123 acc1=0.0903\n",
      "[Epoch 01] loss=2.9917 acc1=0.5313\n",
      "[Epoch 02] loss=1.0810 acc1=0.7930\n",
      "[Epoch 03] loss=0.5342 acc1=0.8864\n",
      "[Epoch 04] loss=0.3555 acc1=0.9221\n",
      "[CKPT] Saved reid_arcface_m0.25_epoch05.pth\n",
      "[Epoch 05] loss=0.2489 acc1=0.9467\n",
      "[Epoch 06] loss=0.2016 acc1=0.9561\n",
      "[Epoch 07] loss=0.2011 acc1=0.9539\n",
      "[Epoch 08] loss=0.1568 acc1=0.9682\n",
      "[Epoch 09] loss=0.1728 acc1=0.9613\n",
      "[CKPT] Saved reid_arcface_m0.25_epoch10.pth\n",
      "[Epoch 10] loss=0.1798 acc1=0.9588\n",
      "[Epoch 11] loss=0.1668 acc1=0.9642\n",
      "[Epoch 12] loss=0.1414 acc1=0.9681\n",
      "[Epoch 13] loss=0.1570 acc1=0.9648\n",
      "[Epoch 14] loss=0.1510 acc1=0.9661\n",
      "[CKPT] Saved reid_arcface_m0.25_epoch15.pth\n",
      "[Epoch 15] loss=0.1359 acc1=0.9675\n",
      "[Epoch 16] loss=0.1492 acc1=0.9646\n",
      "[Epoch 17] loss=0.0573 acc1=0.9879\n",
      "[Epoch 18] loss=0.0250 acc1=0.9951\n",
      "[Epoch 19] loss=0.0192 acc1=0.9962\n",
      "[CKPT] Saved reid_arcface_m0.25_epoch20.pth\n"
     ]
    }
   ],
   "source": [
    "# -------------------- LOSS --------------------\n",
    "label_smoothing = 0.0\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "# -------------------- MODEL --------------------\n",
    "num_classes = len(train_reid_ds.pids)\n",
    "emb_dim = 512\n",
    "arc_s = 30.0\n",
    "arc_m = 0.25\n",
    "\n",
    "model_reid_af_cx = ReIDNetArcFaceConvNeXt( \n",
    "            emb_dim=emb_dim,\n",
    "            num_classes=num_classes,\n",
    "            s=arc_s,\n",
    "            m=arc_m,\n",
    "            variant=\"tiny\"  # the smallest one\n",
    "        ).to(device)\n",
    "\n",
    "# -------------------- OPTIMIZER --------------------\n",
    "optimizer = torch.optim.AdamW(model_reid_af_cx.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# -------------------- LR SCHEDULER: warmup (per-iter) + step drop (per-epoch) --------------------\n",
    "steps_per_epoch = len(train_reid_loader)\n",
    "\n",
    "warmup_epochs = 1\n",
    "warmup_steps = warmup_epochs * steps_per_epoch\n",
    "\n",
    "def warmup_lambda(step: int) -> float:\n",
    "    if step < warmup_steps:\n",
    "        return (step + 1) / max(1, warmup_steps)\n",
    "    return 1.0\n",
    "\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
    "\n",
    "milestones = [16]\n",
    "gamma = 0.1\n",
    "step_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "\n",
    "# -------------------- WANDB INIT --------------------\n",
    "run = wandb.init(\n",
    "    entity=\"unibo-ai\",\n",
    "    project=\"person re-id\",\n",
    "    config={\n",
    "        \"seed\": seed,\n",
    "        \"dataset\": \"PRW\",\n",
    "        \"architecture\": \"ReIDNetArcFaceConvNeXt\",\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"epochs\": n_epochs,\n",
    "        \"batch_size\": getattr(train_reid_loader, \"batch_size\", None),\n",
    "        \"sampler\": type(getattr(train_reid_loader, \"batch_sampler\", None)).__name__\n",
    "        if getattr(train_reid_loader, \"batch_sampler\", None) is not None else \"shuffle\",\n",
    "        \"resize\": \"(256,128)\",\n",
    "        \"loss\": f\"ArcFace + CE (label_smoothing={label_smoothing})\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"learning_rate\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"scheduler\": \"warmup(1 epoch, per-iter) + MultiStepLR(milestone=16, gamma=0.1)\",\n",
    "        \"warmup_epochs\": warmup_epochs,\n",
    "        \"milestones\": milestones,\n",
    "        \"gamma\": gamma,\n",
    "        \"amp\": use_amp,\n",
    "        \"save_every_epochs\": 5,\n",
    "        \"arcface_s\": arc_s,\n",
    "        \"arcface_m\": arc_m,\n",
    "    },\n",
    "    name=f\"reid_CXt_arcface_20e_m25_adamw_warmup_step_{int(time.time())}\",\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "# -------------------- TRAIN LOOP --------------------\n",
    "global_step = 0\n",
    "save_every = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model_reid_af_cx.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc1 = 0.0\n",
    "\n",
    "    for crops, labels, pid, camid in train_reid_loader:\n",
    "        crops = crops.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True).view(-1)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "            logits, emb = model_reid_af_cx(crops, labels=labels)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        # ---- AMP step ----\n",
    "        prev_scale = scaler.get_scale()\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.step(optimizer)   # may be skipped on overflow\n",
    "        scaler.update()\n",
    "\n",
    "        # FIX: step warmup scheduler ONLY if optimizer actually stepped (avoid warning + keep schedule aligned)\n",
    "        new_scale = scaler.get_scale()\n",
    "        optimizer_was_stepped = (new_scale >= prev_scale)\n",
    "\n",
    "        if optimizer_was_stepped and (global_step < warmup_steps):\n",
    "            warmup_scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss_val = float(loss.item())\n",
    "            pred = logits.argmax(dim=1)\n",
    "            acc1 = (pred == labels).float().mean().item()\n",
    "            emb_norm = emb.norm(dim=1).mean().item()\n",
    "            max_prob = F.softmax(logits, dim=1).max(dim=1).values.mean().item()\n",
    "            lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        running_loss += loss_val\n",
    "        running_acc1 += acc1\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train/loss\": loss_val,\n",
    "                \"train/acc1\": acc1,\n",
    "                \"train/emb_norm_mean\": emb_norm,\n",
    "                \"train/max_prob_mean\": max_prob,\n",
    "                \"train/lr\": lr_now,\n",
    "                \"epoch\": epoch,\n",
    "                \"step\": global_step,\n",
    "            },\n",
    "            step=global_step\n",
    "        )\n",
    "        global_step += 1\n",
    "\n",
    "    epoch_loss = running_loss / max(1, len(train_reid_loader))\n",
    "    epoch_acc1 = running_acc1 / max(1, len(train_reid_loader))\n",
    "\n",
    "    wandb.log(\n",
    "        {\"train/epoch_loss_avg\": epoch_loss, \"train/epoch_acc1_avg\": epoch_acc1, \"epoch\": epoch},\n",
    "        step=global_step\n",
    "    )\n",
    "    print(f\"[Epoch {epoch:02d}] loss={epoch_loss:.4f} acc1={epoch_acc1:.4f}\")\n",
    "\n",
    "    # per-epoch step decay after warmup epoch(s)\n",
    "    if (epoch + 1) > warmup_epochs:\n",
    "        step_scheduler.step()\n",
    "\n",
    "    # -------------------- SAVE CHECKPOINTS --------------------\n",
    "    if ((epoch + 1) % save_every) == 0:\n",
    "        ckpt_path = f\"reid_arcface_m{arc_m:g}_epoch{epoch+1:02d}.pth\"  # FIX: cleaner name\n",
    "        ckpt = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"global_step\": global_step,\n",
    "            \"model\": model_reid_af_cx.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"warmup_scheduler\": warmup_scheduler.state_dict(),\n",
    "            \"step_scheduler\": step_scheduler.state_dict(),\n",
    "            \"scaler\": scaler.state_dict() if use_amp else None,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"emb_dim\": emb_dim,\n",
    "            \"pid2label\": train_reid_ds.pid2label,\n",
    "            \"config\": {\n",
    "                \"seed\": seed,\n",
    "                \"lr\": lr,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"label_smoothing\": label_smoothing,\n",
    "                \"n_epochs\": n_epochs,\n",
    "                \"use_amp\": use_amp,\n",
    "                \"save_every\": save_every,\n",
    "                \"warmup_epochs\": warmup_epochs,\n",
    "                \"milestones\": milestones,\n",
    "                \"gamma\": gamma,\n",
    "                \"arcface_s\": arc_s,\n",
    "                \"arcface_m\": arc_m,\n",
    "            },\n",
    "        }\n",
    "        torch.save(ckpt, ckpt_path)\n",
    "\n",
    "        artifact = wandb.Artifact(f\"reid_arcface_m{arc_m:g}_epoch{epoch+1:02d}\", type=\"model\")\n",
    "        artifact.add_file(ckpt_path)\n",
    "        wandb.log_artifact(artifact)\n",
    "        print(f\"[CKPT] Saved {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T10:18:21.219829Z",
     "iopub.status.busy": "2026-02-04T10:18:21.219195Z",
     "iopub.status.idle": "2026-02-04T10:52:27.362952Z",
     "shell.execute_reply": "2026-02-04T10:52:27.362349Z",
     "shell.execute_reply.started": "2026-02-04T10:18:21.219800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Eval] Loading checkpoint: /kaggle/input/arcface-convnext-weights/reid_cx_arcface_m25_epoch20.pth\n",
      "search ranking:\n",
      "  mAP = 47.55%\n",
      "  top- 1 = 84.74%\n",
      "[Eval] /kaggle/input/arcface-convnext-weights/reid_cx_arcface_m25_epoch20.pth  mAP=47.55% | top-1=84.74%\n",
      "\n",
      "[Eval] Loading checkpoint: /kaggle/input/arcface-convnext-weights/reid_cx_arcface_m25_epoch10.pth\n",
      "search ranking:\n",
      "  mAP = 48.51%\n",
      "  top- 1 = 85.90%\n",
      "[Eval] /kaggle/input/arcface-convnext-weights/reid_cx_arcface_m25_epoch10.pth  mAP=48.51% | top-1=85.90%\n",
      "\n",
      "[Eval] Loading checkpoint: /kaggle/input/arcface-convnext-weights/reid_cx_arcface_m25_epoch15.pth\n",
      "search ranking:\n",
      "  mAP = 45.97%\n",
      "  top- 1 = 84.05%\n",
      "[Eval] /kaggle/input/arcface-convnext-weights/reid_cx_arcface_m25_epoch15.pth  mAP=45.97% | top-1=84.05%\n",
      "\n",
      "[Eval] Loading checkpoint: /kaggle/input/arcface-convnext-weights/reid_cx_arcface_m25_epoch05.pth\n",
      "search ranking:\n",
      "  mAP = 49.69%\n",
      "  top- 1 = 86.58%\n",
      "[Eval] /kaggle/input/arcface-convnext-weights/reid_cx_arcface_m25_epoch05.pth  mAP=49.69% | top-1=86.58%\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "num_classes = len(train_reid_ds.pids)\n",
    "emb_dim = 512\n",
    "model_reid_af_cx = ReIDNetArcFaceConvNeXt( \n",
    "            emb_dim=emb_dim,\n",
    "            num_classes=num_classes,\n",
    "            s=arc_s,\n",
    "            m=arc_m,\n",
    "            variant=\"tiny\"  # the smallest one\n",
    "        ).to(device)\n",
    "\n",
    "# Load ckpts\n",
    "ckpt_epochs = [20, 10, 15, 5]\n",
    "ckpt_paths  = [f\"/kaggle/input/arcface-convnext-weights/reid_cx_arcface_m25_epoch{e:02d}.pth\" for e in ckpt_epochs]\n",
    "\n",
    "# Eval\n",
    "results = {}\n",
    "for epoch, ckpt_path in zip(ckpt_epochs, ckpt_paths):\n",
    "    ret = evaluate_checkpoint_prw(\n",
    "        ckpt_path=ckpt_path,\n",
    "        model=model_reid_af_cx,\n",
    "        query_ds=query_ds,\n",
    "        gallery_eval=gallery_eval,\n",
    "        test_ds=test_ds,\n",
    "        test_detections=test_detections,\n",
    "        test_reid_tf=test_reid_tf,\n",
    "        device=device,\n",
    "    )\n",
    "    results[epoch] = ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considerations\n",
    "Our results highlights the importance of the loss choice and the domain in which it operates (CE vs CosFace vs ArcFace, angular domain); the feature extractor (ConvNeXt-Tiny vs ResNet50) and a further finetuning using contrastive losses (Triplet and InfoNCE losses)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8908164,
     "sourceId": 13973040,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9271237,
     "sourceId": 14515974,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9421117,
     "sourceId": 14741572,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9428607,
     "sourceId": 14752233,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9430040,
     "sourceId": 14754193,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9430223,
     "sourceId": 14754433,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9453794,
     "sourceId": 14787782,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
